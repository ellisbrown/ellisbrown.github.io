<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Gender Inference from Character Sequences in Multinational First Names | Ellis Brown </title> <meta name="author" content="Ellis L. Brown II"> <meta name="description" content="Accurate prediction of an unknown individual’s gender is desirable for use in marketing, social science, and many other applications in academia and industry. Perhaps the most obvious and telling indicator of a person’s gender is their first name."> <meta name="keywords" content="self-supervised learning, representation learning, multimodal, vision-language, generalization, curiosity, deep learning, computer vision, machine learning, artificial intelligence"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%B4&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ellisbrown.github.io//blog/2017/name2gender-introduction/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Ellis Brown </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">research </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Gender Inference from Character Sequences in Multinational First Names</h1> <p class="post-meta"> December 26, 2017 </p> <p class="post-tags"> <a href="/blog/2017"> <i class="fa-solid fa-calendar fa-sm"></i> 2017 </a>   ·   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a>   <a href="/blog/tag/pytorch"> <i class="fa-solid fa-hashtag fa-sm"></i> PyTorch</a>     ·   <a href="/blog/category/nlp"> <i class="fa-solid fa-tag fa-sm"></i> NLP</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote> <p><em>Note: This post has also been published on Medium <a href="https://towardsdatascience.com/name2gender-introduction-626d89378fb0" rel="external nofollow noopener" target="_blank">here</a>.</em></p> </blockquote> <p>Consider the names “John” and “Cindy” — most people would instantly mark John as a male name and Cindy as a female one. Is this the case <em>primarily</em> because we have seen so many examples of male Johns and female Cindys that our brains have built up a latent association between the specific name and the corresponding gender? Probably.</p> <p>But some component of the name itself (its spelling / combination of letters) contributes to the gender with which it is associated to a large degree as well. Consider the names “Andy” and “Andi.” They are phonetically identical (<a href="https://en.wiktionary.org/wiki/Andy#Pronunciation" rel="external nofollow noopener" target="_blank">/ˈæn.di/</a>), however most people would categorize “Andy” as male and “Andi” as female upon seeing the spellings. The suffix of a name can indicate the name’s gender; however, the rules are not cut and dry. For example, names “ending in <em>-yn</em> appear to be predominantly female, despite the fact that names ending in <em>-n</em> tend to be male; and names ending in <em>-ch</em> are usually male, even though names that end in <em>-h</em> tend to be female.” <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> There are many more character patterns that correspond to a certain gender classification than just the suffix — this task is not trivial.</p> <p>The gender classification of a name becomes increasingly difficult when you consider the space of all names from around the world — the examples I have given thus far are admittedly from a standard American viewpoint. Let’s now consider two Indian names (a culture with which I have next to no exposure): when I see the names “Priyanka” and “Srikanth,” I instantly assume Priyanka to be female and Srikanth to be male. Why is that? How do our brains extract the gender revealing information encoded in the sequence of characters that compose a name?</p> <h1 id="who-cares">Who cares?</h1> <p>Accurate prediction of an unknown individual’s gender is desirable for use in marketing, social science, and many other applications in academia and industry. Perhaps the most obvious and telling indicator of a person’s gender is their first name. Most previous work in classifying gender via first name has concerned using a large corpus of known names to give a probabilistic prediction on the names that are known. This post attempts to explore the space of names that are <em>unknown</em> by examining the facets of a first name — specifically focusing on the sequences of characters within the name — that contain non-trivial gender-revealing information. It is also an exercise in applying ML/DL to real problems.</p> <p>This gender classification problem is similar to but fundamentally different from a larger class of problem in <a href="https://en.wikipedia.org/wiki/Natural_language_understanding" rel="external nofollow noopener" target="_blank">Natural Language Understanding</a>. Given the words “cat” and “dog,” almost all people will assign cat to female and dog to male.</p> <figure class="align-left"> <img src="/assets/posts/name2gen/dog.jpg" alt="Dog"> <figcaption><a href="https://goo.gl/YZPEjm" rel="external nofollow noopener" target="_blank">source</a></figcaption> </figure> <p>The core difference here is that when we read the word “dog,” our brains translate the sequence of characters “d-o-g” to a high dimensional representation of the abstract entity that is our understanding of a dog. It is some combination of features in this high dimensional representation in our brains that correlate more closely to our abstract representation of <em>males</em> than to that of <em>females</em>; it is not the sequence of characters themselves that contain the gender-revealing information (at least for the most part).</p> <p>Our gender classification problem becomes even more interesting when you abstract it from names to <em>all</em> words in general. Linguistically, many languages are structured with the concept of a <a href="https://en.wikipedia.org/wiki/Grammatical_gender" rel="external nofollow noopener" target="_blank">grammatical gender</a> wherein classes of nouns in the language are formally associated with one of a discrete set of genders.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> In such languages, certain sequences of characters in a word can almost surely identify the grammatical gender of the word. Learning these character sequences — whether explicitly or implicitly — is an inherent part of learning the language. Furthermore, understanding of such character patterns may assist in understanding of unseen words. For these reasons, studying information embedded in character sequences seems like an interesting and integral topic to Linguistics and NLU that is beyond the scope of this post.</p> <hr> <h1 id="methodology">Methodology</h1> <p>All code for this project is available at</p> <div class="repositories d-flex flex-wrap flex-md-row flex-column justify-content-between align-items-center"> <div class="repo p-2 text-center"> <a href="https://github.com/ellisbrown/name2gender" rel="external nofollow noopener" target="_blank"> <img class="repo-img-light w-100" alt="ellisbrown/name2gender" src="https://github-readme-stats.vercel.app/api/pin/?username=ellisbrown&amp;repo=name2gender&amp;theme=default&amp;show_owner=false"> <img class="repo-img-dark w-100" alt="ellisbrown/name2gender" src="https://github-readme-stats.vercel.app/api/pin/?username=ellisbrown&amp;repo=name2gender&amp;theme=dark&amp;show_owner=false"> </a> </div> </div> <p><br></p> <h2 id="naïve-bayes">Naïve-Bayes</h2> <p>As an initial approach to the topic, I explore a vanilla machine learning technique using hard-coding features of names that are known to have high correlation to the name’s associated gender (such as suffix, as mentioned earlier). This quick and dirty implementation is actually able to achieve pretty good results with minimal work.</p> <blockquote> <p>The features in used in this approach are pulled directly from the NLTK book: Names ending in -a, -e and -i are likely to be female, while names ending in -k, -o, -r, -s and -t are likely to be male… names ending in -yn appear to be predominantly female, despite the fact that names ending in -n tend to be male; and names ending in -ch are usually male, even though names that end in -h tend to be female.<sup id="fnref:1:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p> </blockquote> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">gender_features</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
<span class="n">features</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">features</span><span class="p">[</span><span class="sh">"</span><span class="s">last_letter</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">name</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">lower</span><span class="p">()</span>
<span class="n">features</span><span class="p">[</span><span class="sh">"</span><span class="s">first_letter</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">name</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">lower</span><span class="p">()</span> <span class="c1"># names ending in -yn are mostly female, names ending in -ch are mostly male, so add 3 more features
</span><span class="n">features</span><span class="p">[</span><span class="sh">"</span><span class="s">suffix2</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">name</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
<span class="n">features</span><span class="p">[</span><span class="sh">"</span><span class="s">suffix3</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">name</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span>
<span class="n">features</span><span class="p">[</span><span class="sh">"</span><span class="s">suffix4</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">name</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:]</span>
<span class="k">return</span> <span class="n">features</span></code></pre></figure> <p>These features are plugged into the NLTK NaiveBayesClassifier for easy training and testing.</p> <h3 id="results">Results</h3> <p>I trained this model with a 70/30 train-test split (~95k training names, ~40.6k testing names). The testing accuracy was around <strong>85%</strong>, which is arguably pretty good for this task.</p> <p>Example code usage is available at <a href="https://github.com/ellisbrown/name2gender/blob/master/naive_bayes/demo.ipynb" rel="external nofollow noopener" target="_blank">naive_bayes/demo.ipynb</a>.</p> <h2 id="char-rnn">Char-RNN</h2> <p>Of course, there are many many patterns of characters in a name that might contain gender cues — especially when considering our global (worldly) name space; it seems absurd to attempt to hard code every possible pattern when we have <a href="https://en.wikipedia.org/wiki/Deep_learning" rel="external nofollow noopener" target="_blank">Deep Learning</a>. In this vein, I explore a Character-level Recurrent Neural Network approach using <a href="http://pytorch.org/" rel="external nofollow noopener" target="_blank">PyTorch</a> that attempts to learn the various gender-revealing sequences without having to explicitly specify them.</p> <h3 id="tensor-representation">Tensor Representation</h3> <p>The first step here is to figure out how to represent a name as a <a href="https://en.wikipedia.org/wiki/Tensor" rel="external nofollow noopener" target="_blank">tensor</a>. Since our goal is to pick up on all of the nuances in the sequences of letters that make up first names, we want to break up the name and look character by character. In order to represent each character, we create a <a href="https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f" rel="external nofollow noopener" target="_blank">one-hot vector</a> of size <code class="language-plaintext highlighter-rouge">&lt;1 x N_LETTERS&gt;</code> (a one-hot vector is filled with 0s except for a 1 at the index of the current letter, e.g. <code class="language-plaintext highlighter-rouge">"c" = &lt;0 0 1 0 0 ... 0&gt;</code>).</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">name_to_tensor</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">cuda</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
<span class="sh">"""</span><span class="s">converts a name to a vectorized numerical input for use with a nn
each character is converted to a one hot (n, 1, 26) tensor
Args:
name (string): first name (e.g., </span><span class="sh">"</span><span class="s">Ellis</span><span class="sh">"</span><span class="s">)
Return:
tensor (torch.tensor)
</span><span class="sh">"""</span>
<span class="n">name</span> <span class="o">=</span> <span class="nf">clean_str</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">FloatTensor</span> <span class="k">if</span> <span class="n">cuda</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">N_LETTERS</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">tensor</span><span class="p">)</span>
<span class="k">for</span> <span class="n">li</span><span class="p">,</span> <span class="n">letter</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
<span class="n">letter_index</span> <span class="o">=</span> <span class="n">ALL_LETTERS</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="n">letter</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">[</span><span class="n">li</span><span class="p">][</span><span class="n">letter_index</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">return</span> <span class="n">tensor</span></code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">tensor_to_name</span><span class="p">(</span><span class="n">name_tensor</span><span class="p">):</span>
<span class="sh">"""</span><span class="s">converts a name tensor to a string representation of a name
Args:
tensor (torch.tensor)
Return:
name (string)
</span><span class="sh">"""</span>
<span class="n">ret</span> <span class="o">=</span> <span class="sh">""</span>
<span class="k">for</span> <span class="n">letter_tensor</span> <span class="ow">in</span> <span class="n">name_tensor</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
<span class="n">nz</span> <span class="o">=</span> <span class="n">letter_tensor</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">nonzero</span><span class="p">()</span>
<span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="nf">numel</span><span class="p">(</span><span class="n">nz</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
<span class="n">ret</span> <span class="o">+=</span> <span class="p">(</span><span class="n">string</span><span class="p">.</span><span class="n">ascii_lowercase</span><span class="p">[</span><span class="n">nz</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="k">return</span> <span class="n">ret</span></code></pre></figure> <h3 id="nametensor-conversion">Name/Tensor conversion</h3> <p>An alternative approach might be to store the value of a character by its location in the alphabet <code class="language-plaintext highlighter-rouge">"c" = 3</code>; however, this might cause the model to learn some patterns in the value of the character that we do not intend (e.g., it might learn that “c” is more similar to “a” than it is to “x” because they are closer alphabetically while there is actually no similarity difference).</p> <figure class="align-right" style="max-width: 200px"> <img src="/assets/posts/name2gen/rnn.png" alt="RNN" style="max-width: 100%" data-zoomable=""> <figcaption><a href="https://goo.gl/BB7h2A" rel="external nofollow noopener" target="_blank">source</a></figcaption> </figure> <h3 id="model-definition">Model Definition</h3> <p>We then define the structure of the network module itself:</p> <p>Following the direction of the <a href="https://goo.gl/BB7h2A" rel="external nofollow noopener" target="_blank">PyTorch name nationality classification example</a>, we create a simple network with 2 <a href="http://pytorch.org/docs/master/nn.html#linear-layers" rel="external nofollow noopener" target="_blank">linear layers</a> operating on an input and hidden state, and a <a href="http://pytorch.org/docs/master/nn.html#logsoftmax" rel="external nofollow noopener" target="_blank">LogSoftmax</a> layer on the output. I use 128 hidden units.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">3</a></sup></p> <p>This is a very simple network definition, and likely could be improved by adding more linear layers or better shaping the network.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
<span class="sh">"""</span><span class="s">Recurrent Neural Network
original source: https://goo.gl/12wiKB
Simple implementation of an RNN with two linear layers and a LogSoftmax
layer on the output
Args:
input_size: (int) size of data
hidden_size: (int) number of hidden units
output_size: (int) size of output
</span><span class="sh">"""</span>
<span class="k">def</span> <span class="err">**</span><span class="nf">init</span><span class="o">**</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="nf">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="o">**</span><span class="n">init</span><span class="o">**</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>

        <span class="n">self</span><span class="p">.</span><span class="n">i2h</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">i2o</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="nb">input</span><span class="p">.</span><span class="nf">float</span><span class="p">(),</span> <span class="n">hidden</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">i2h</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">i2o</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>

    <span class="k">def</span> <span class="nf">init_hidden</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">cuda</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">cuda</span><span class="p">:</span>
            <span class="n">ret</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>

<span class="k">return</span> <span class="nc">Variable</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span></code></pre></figure> <h3 id="results-1">Results</h3> <p>I again split the dataset with a 70/30 train-test split (~95k training names, ~40.6k testing names). The best testing accuracy I was able to achieve was around <strong>75.4% accuracy</strong>. I did not spend much time tweaking hyper-parameters for better results.</p> <h2 id="dataset--name2genderdata">Dataset — <a href="https://github.com/ellisbrown/name2gender/tree/master/data" rel="external nofollow noopener" target="_blank">name2gender/data/</a> </h2> <p>I have extended NLTK’s names corpus with many more datasets representing various cultures into a large dataset (~135k instances) of gender-labeled first names, which is available on my repository. See <a href="https://github.com/ellisbrown/name2gender/blob/master/data/dataset.ipynb" rel="external nofollow noopener" target="_blank">data/dataset.ipynb</a> for further information on how I pulled it together. <em>Note:</em> I did not spend a ton of time going through and pruning this dataset, so it is probably not amazing or particularly clean (I would greatly appreciate any PR’s if anyone cares or has the time!).</p> <p>Improving / cleaning this dataset would likely be the most impactful improvement initially. Additionally, using a dataset of only names from a single culture would likely be much better at predicting names in that culture.</p> <hr> <h2 id="disclaimer">Disclaimer</h2> <p>It is worthy to acknowledge that there are many names, such as my own (Ellis), that are about equally common among both genders. Datasets containing discrete labels, as opposed to frequency of occurrence in the world, were much easier to come across, and so I have only taken into consideration the binary classification of names given by these datasets. A more robust approach would incorporate the frequency of occurrence in a population to give a more probabilistic gender prediction. The “Gender-name association scores” approach by Wendy Liu might be a good approach.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">4</a></sup> The presence of gender ambiguous names also limits the best case real-world accuracy that could be achieved by a gender classification system.</p> <p>Furthermore, there are increasing movements being made to recognize more than the traditional binary male and female genders in our society. As these efforts are still in their infancy and there is very little (if any) data containing these expanded gender classifications, I make no attempt to incorporate them into this analysis.</p> <p>I also only take names written in the Latin Alphabet into consideration. Abstracting to entirely language agnostic classification is too formidable a task for the sake of this post.</p> <h2 id="future-work">Future Work</h2> <p>As I touched on above, improving the dataset is definitely the best starting point for improvements. Application of this name to gender classification to variations of first names would be a really useful to anyone who finds first name classification useful. As Wendy Liu puts it:</p> <blockquote> <p>“Nicknames, abbreviations, mangled names, and usernames can frequently contain non-trivial gender cues. Identifying strategies for extracting and using these cues to more accurately infer gender is a promising direction for future work.” <sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">4</a></sup></p> </blockquote> <figure class="align-right" style="max-width: 280px"> <img src="/assets/posts/name2gen/freq.png" alt="Freq" style="max-width: 100%" data-zoomable=""> <figcaption>Name Frequency Scheme</figcaption> <br> </figure> <p>There is also the possibility of crafting the testing scheme to better represent a real world use case. In my analysis, I treated every name as equally likely to occur. A better real world dataset might include some representation of how frequently a certain name occurs in the world. For example, “John” would have a much higher frequency rating than “Ellis.” This change would affect how the testing results are calculated, and the system as a whole. We would ideally have most of the common first names included in our dataset.</p> <p>For such names, we could simply lookup what our datastore has for the gender of the name. Our predictive system would then only be applied to the set of names that are not common. To test this system setup, we could sort the names by frequency rating, and leave the 30% of names that are least common for the test set.</p> <p><br> <br></p> <hr> <p><br></p> <h2 id="references">References</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>Bird, S., Klein, E., and Loper, E. “6.1.1 Gender Identification.” Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit, O’Reilly, 2009, <a href="https://www.nltk.org/book/ch06.html" style="word-wrap:break-word;" rel="external nofollow noopener" target="_blank">https://www.nltk.org/book/ch06.html</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">↩<sup>2</sup></a></p> </li> <li id="fn:2" role="doc-endnote"> <p>“Grammatical Gender.” Wikipedia, Wikimedia Foundation, 21 Dec. 2017, <a href="http://en.wikipedia.org/wiki/Grammatical_gender" style="word-wrap:break-word;" rel="external nofollow noopener" target="_blank">http://en.wikipedia.org/wiki/Grammatical_gender</a>. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:4" role="doc-endnote"> <p>Robertson, S. “Classifying Names with a Character-Level RNN.” Classifying Names with a Character-Level RNN, PyTorch Docs, 2017, <a href="http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html" style="word-wrap:break-word;" rel="external nofollow noopener" target="_blank">http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html</a>. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:3" role="doc-endnote"> <p>Liu, W., and Ruths, D. “What’s in a Name? Using First Names as Features for Gender Inference in Twitter” AAAI Spring Symposium Series (2013), 21 Dec. 2017, <a href="https://www.semanticscholar.org/paper/What%27s-in-a-Name-Using-First-Names-as-Features-for-Liu-Ruths/b60d04043a60e46670f182b2debb485e9d17ce46?utm_source=direct_link" style="word-wrap:break-word;" rel="external nofollow noopener" target="_blank">link</a>. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">↩</a> <a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">↩<sup>2</sup></a></p> </li> </ol> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Ellis L. Brown II. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: June 13, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-00N9EWF2WT"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-00N9EWF2WT");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>