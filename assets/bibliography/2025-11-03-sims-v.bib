
@misc{bennett2010openstreetmap,
  title     = {{OpenStreetMap}},
  publisher = {Bennett, Jonathan},
  author    = {Bennett, Jonathan},
  year      = {2010}
}

@misc{google_map_platform,
  title        = {Google {M}ap {P}latform},
  howpublished = {\url{https://mapsplatform.google.com/}},
  publisher    = {{Google Map Team}},
  author       = {{Google Map Team}}
}

@inproceedings{radford2021learning,
  title     = {Learning transferable visual models from natural language supervision},
  author    = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle = {ICML},
  year      = {2021}
}

@inproceedings{li2022grounded,
  title     = {{Grounded language-image pre-training}},
  author    = {Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and others},
  booktitle = {CVPR},
  year      = {2022}
}



@article{liu2023grounding,
  title   = {{Grounding DINO}: Marrying dino with grounded pre-training for open-set object detection},
  author  = {Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},
  journal = {arXiv preprint arXiv:2303.05499},
  year    = {2023}
}

@article{openai2023gpt,
  title   = {{GPT-4 technical report}},
  author  = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal = {arXiv preprint arXiv:2303.08774},
  year    = {2023}
}

@inproceedings{li2023blip,
  title     = {{BLIP-2:} Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author    = {Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
  year      = {2023},
  booktitle = {ICML}
}

@misc{zhen20243dvla3dvisionlanguageactiongenerative,
      title={3D-VLA: A 3D Vision-Language-Action Generative World Model},
      author={Haoyu Zhen and Xiaowen Qiu and Peihao Chen and Jincheng Yang and Xin Yan and Yilun Du and Yining Hong and Chuang Gan},
      year={2024},
      eprint={2403.09631},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.09631},
}

@article{cho2024language,
      title={Language-Image Models with 3D Understanding},
      author={Cho, Jang Hyun and Ivanovic, Boris and Cao, Yulong and Schmerling, Edward and Wang, Yue and Weng, Xinshuo and Li, Boyi and You, Yurong and Kr{\"a}henb{\"u}hl, Philipp and Wang, Yan and others},
      journal={arXiv preprint arXiv:2405.03685},
      year={2024}
    }

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@misc{kakaobrain2022coyo-700m,
  title         = {COYO-700M: Image-Text Pair Dataset},
  author        = {Byeon, Minwoo and Park, Beomhee and Kim, Haecheon and Lee, Sungjun and Baek, Woonhyuk and Kim, Saehoon},
  year          = {2022},
  howpublished  = {\url{https://github.com/kakaobrain/coyo-dataset}},
}

@misc{gadre2023datacompsearchgenerationmultimodal,
      title={DataComp: In search of the next generation of multimodal datasets},
      author={Samir Yitzhak Gadre and Gabriel Ilharco and Alex Fang and Jonathan Hayase and Georgios Smyrnis and Thao Nguyen and Ryan Marten and Mitchell Wortsman and Dhruba Ghosh and Jieyu Zhang and Eyal Orgad and Rahim Entezari and Giannis Daras and Sarah Pratt and Vivek Ramanujan and Yonatan Bitton and Kalyani Marathe and Stephen Mussmann and Richard Vencu and Mehdi Cherti and Ranjay Krishna and Pang Wei Koh and Olga Saukh and Alexander Ratner and Shuran Song and Hannaneh Hajishirzi and Ali Farhadi and Romain Beaumont and Sewoong Oh and Alex Dimakis and Jenia Jitsev and Yair Carmon and Vaishaal Shankar and Ludwig Schmidt},
      year={2023},
      eprint={2304.14108},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.14108},
}

@inproceedings{shao2019objects365,
  title={Objects365: A large-scale, high-quality dataset for object detection},
  author={Shao, Shuai and Li, Zeming and Zhang, Tianyuan and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Li, Jing and Sun, Jian},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={8430--8439},
  year={2019}
}

@misc{changpinyo2021conceptual12mpushingwebscale,
      title={Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts},
      author={Soravit Changpinyo and Piyush Sharma and Nan Ding and Radu Soricut},
      year={2021},
      eprint={2102.08981},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2102.08981},
}

@book{gardner2011frames,
  title={Frames of mind: The theory of multiple intelligences},
  author={Gardner, Howard E},
  year={2011},
  publisher={Basic books}
}

@article{tversky2009thinking,
  title={Thinking with sketches.},
  author={Tversky, Barbara and Suwa, Masaki},
  year={2009},
  publisher={Oxford University Press}
}



@inproceedings{achlioptas2020referit3d,
  title={Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes},
  author={Achlioptas, Panos and Abdelreheem, Ahmed and Xia, Fei and Elhoseiny, Mohamed and Guibas, Leonidas},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part I 16},
  pages={422--440},
  year={2020},
  organization={Springer}
}


@article{blades1994development,
  title={The development of children's ability to use spatial representations},
  author={Blades, Mark and Spencer, Christopher},
  journal={Advances in child development and behavior},
  volume={25},
  pages={157--199},
  year={1994},
  publisher={Elsevier}
}

@article{brucato2023measuring,
  title={Measuring spatial perspective taking: Analysis of four measures using item response theory},
  author={Brucato, Maria and Frick, Andrea and Pichelmann, Stefan and Nazareth, Alina and Newcombe, Nora S},
  journal={Topics in Cognitive Science},
  volume={15},
  number={1},
  pages={46--74},
  year={2023},
  publisher={Wiley Online Library}
}


@book{tversky2019mind,
  title={Mind in motion: How action shapes thought},
  author={Tversky, Barbara},
  year={2019},
  publisher={Hachette UK}
}

@article{vasilyeva2012development,
  title={Development of spatial cognition},
  author={Vasilyeva, Marina and Lourenco, Stella F},
  journal={Wiley Interdisciplinary Reviews: Cognitive Science},
  volume={3},
  number={3},
  pages={349--362},
  year={2012},
  publisher={Wiley Online Library}
}

@book{kahneman2011thinking,
  title={Thinking, fast and slow},
  author={Kahneman, Daniel},
  year={2011},
  publisher={macmillan}
}


@inproceedings{liu2023visual,
  title     = {Visual instruction tuning},
  author    = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  booktitle = {NeurIPS},
  year      = {2023}
}

@inproceedings{dai2023instructblip,
  title     = {{InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning}},
  author    = {Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
  year      = {2023},
  booktitle = {NeurIPS}
}

@inproceedings{chen2020scanrefer,
  title={Scanrefer: 3d object localization in rgb-d scans using natural language},
  author={Chen, Dave Zhenyu and Chang, Angel X and Nie{\ss}ner, Matthias},
  booktitle={European conference on computer vision},
  pages={202--221},
  year={2020},
  organization={Springer}
}

@misc{su202125dvisualrelationshipdetection,
      title={2.5D Visual Relationship Detection},
      author={Yu-Chuan Su and Soravit Changpinyo and Xiangning Chen and Sathish Thoppay and Cho-Jui Hsieh and Lior Shapira and Radu Soricut and Hartwig Adam and Matthew Brown and Ming-Hsuan Yang and Boqing Gong},
      year={2021},
      eprint={2104.12727},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2104.12727},
}

@article{EVA-CLIP,
  title   = {{EVA-CLIP: Improved Training Techniques for CLIP at Scale}},
  author  = {Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},
  journal = {arXiv preprint arXiv:2303.15389},
  year    = {2023}
}

@inproceedings{openclip2023,
  title     = {Reproducible scaling laws for contrastive language-image learning},
  author    = {Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
  booktitle = {CVPR},
  year      = {2023}
}

@article{zhai2023siglip,
  title   = {Sigmoid loss for language image pre-training},
  author  = {Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  journal = {arXiv preprint arXiv:2303.15343},
  year    = {2023}
}

@article{zhu2023minigpt,
  title   = {{MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models}},
  author  = {Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal = {arXiv preprint arXiv:2304.10592},
  year    = {2023}
}

@article{chen2023shikra,
  title   = {Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic},
  author  = {Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui},
  journal = {arXiv preprint arXiv:2306.15195},
  year    = {2023}
}


@article{ye2023mplug,
  title   = {{mPLUG-Owl: Modularization empowers large language models with multimodality}},
  author  = {Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others},
  journal = {arXiv preprint arXiv:2304.14178},
  year    = {2023}
}

@article{liu2023improvedllava,
  title   = {Improved Baselines with Visual Instruction Tuning},
  author  = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  journal = {arXiv:2310.03744},
  year    = {2023}
}

@article{liu2023mmbench,
  title   = {{MMBench: Is Your Multi-modal Model an All-around Player?}},
  author  = {Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  journal = {arXiv preprint arXiv:2307.06281},
  year    = {2023}
}

@article{schulman2022chatgpt,
  title   = {{ChatGPT: Optimizing language models for dialogue}},
  author  = {Schulman, John and Zoph, Barret and Kim, Christina and Hilton, Jacob and Menick, Jacob and Weng, Jiayi and Uribe, Juan Felipe Ceron and Fedus, Liam and Metz, Luke and Pokorny, Michael and others},
  journal = {OpenAI blog},
  year    = {2022}
}

@article{xi2023rise,
  title   = {The rise and potential of large language model based agents: A survey},
  author  = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and others},
  journal = {arXiv preprint arXiv:2309.07864},
  year    = {2023}
}

@article{wooldridge1995intelligent,
  title   = {{Intelligent Agents}: Theory and practice},
  author  = {Wooldridge, Michael and Jennings, Nicholas R},
  journal = {The knowledge engineering review},
  year    = {1995}
}

@inproceedings{chen2022visualgpt,
  title     = {{VisualGPT}: Data-efficient adaptation of pretrained language models for image captioning},
  author    = {Chen, Jun and Guo, Han and Yi, Kai and Li, Boyang and Elhoseiny, Mohamed},
  booktitle = {CVPR},
  year      = {2022}
}

@article{wang2023voyager,
  title   = {Voyager: An open-ended embodied agent with large language models},
  author  = {Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  journal = {arXiv preprint arXiv:2305.16291},
  year    = {2023}
}

@inproceedings{huang2022language,
  title     = {{Language Models As Zero-Shot Planners}: Extracting actionable knowledge for embodied agents},
  author    = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle = {ICML},
  year      = {2022}
}

@inproceedings{huang2022inner,
  title     = {{Inner Monologue: Embodied reasoning through planning with language models}},
  author    = {Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and others},
  booktitle = {CoRL},
  year      = {2022}
}
@inproceedings{brohan2023can,
  title     = {{Do As I Can, Not As I Say}: Grounding language in robotic affordances},
  author    = {Brohan, Anthony and Chebotar, Yevgen and Finn, Chelsea and Hausman, Karol and Herzog, Alexander and Ho, Daniel and Ibarz, Julian and Irpan, Alex and Jang, Eric and Julian, Ryan and others},
  booktitle = {CoRL},
  year      = {2023}
}

@inproceedings{park2023generative,
  title     = {{Generative Agents}: Interactive simulacra of human behavior},
  author    = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
  booktitle = {UIST},
  year      = {2023}
}

@article{du2009pp,
  title   = {{PP-OCR}: A practical ultra lightweight OCR system. arXiv 2020},
  author  = {Du, Y and Li, C and Guo, R and Yin, X and Liu, W and Zhou, J and Bai, Y and Yu, Z and Yang, Y and Dang, Q and others},
  journal = {arXiv preprint arXiv:2009.09941},
  year    = {2020}
}

@misc{Significant_Gravitas_AutoGPT,
  title        = {{AutoGPT}},
  author       = {{Significant Gravitas}},
  howpublished = {\url{https://github.com/Significant-Gravitas/AutoGPT}},
  year         = {2023}
}

@article{nakano2021webgpt,
  title   = {{WebGPT: Browser-assisted question-answering with human feedback}},
  author  = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal = {arXiv preprint arXiv:2112.09332},
  year    = {2021}
}

@misc{ranasinghe2024learninglocalizeobjectsimproves,
      title={Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs},
      author={Kanchana Ranasinghe and Satya Narayan Shukla and Omid Poursaeed and Michael S. Ryoo and Tsung-Yu Lin},
      year={2024},
      eprint={2404.07449},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.07449},
}

@inproceedings{schick2023toolformer,
  title     = {Toolformer: Language models can teach themselves to use tools},
  author    = {Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  booktitle = {NeurIPS},
  year      = {2023}
}

@article{yuan2021florence,
  title   = {Florence: A new foundation model for computer vision},
  author  = {Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and others},
  journal = {arXiv preprint arXiv:2111.11432},
  year    = {2021}
}

@inproceedings{alayrac2022flamingo,
  title     = {Flamingo: a visual language model for few-shot learning},
  author    = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  booktitle = {NeurIPS},
  year      = {2022}
}

@inproceedings{li2022languagedriven,
  title     = {Language-driven Semantic Segmentation},
  author    = {Boyi Li and Kilian Q Weinberger and Serge Belongie and Vladlen Koltun and Rene Ranftl},
  booktitle = {ICLR},
  year      = {2022}
}

@inproceedings{ramaswamy2023geode,
  title     = {{GeoDE}: a geographically diverse evaluation dataset for object recognition},
  author    = {Ramaswamy, Vikram V and Lin, Sing Yu and Zhao, Dora and Adcock, Aaron Bryan and van der Maaten, Laurens and Ghadiyaram, Deepti and Russakovsky, Olga},
  booktitle = {NeurIPS},
  year      = {2023}
}

@inproceedings{deng2009imagenet,
  title     = {Image{N}et: A large-scale hierarchical image database},
  author    = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle = {CVPR},
  year      = {2009}
}

@article{kuznetsova2020open,
  title   = {{The Open Images Dataset V4}: Unified image classification, object detection, and visual relationship detection at scale},
  author  = {Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and others},
  journal = {IJCV},
  year    = {2020}
}

@inproceedings{dubey2021adaptive,
  title     = {Adaptive methods for real-world domain generalization},
  author    = {Dubey, Abhimanyu and Ramanathan, Vignesh and Pentland, Alex and Mahajan, Dhruv},
  booktitle = {CVPR},
  year      = {2021}
}

@inproceedings{asano2021pass,
  title     = {{PASS}: An imagenet replacement for self-supervised pretraining without humans},
  author    = {Asano, Yuki M and Rupprecht, Christian and Zisserman, Andrew and Vedaldi, Andrea},
  booktitle = {NeurIPS},
  year      = {2021}
}


@inproceedings{savva2019habitat,
  title     = {Habitat: A platform for embodied ai research},
  author    = {Savva, Manolis and Kadian, Abhishek and Maksymets, Oleksandr and Zhao, Yili and Wijmans, Erik and Jain, Bhavana and Straub, Julian and Liu, Jia and Koltun, Vladlen and Malik, Jitendra and others},
  booktitle = {ICCV},
  year      = {2019}
}

@inproceedings{xia2018gibson,
  title     = {{Gibson Env}: Real-world perception for embodied agents},
  author    = {Xia, Fei and Zamir, Amir R and He, Zhiyang and Sax, Alexander and Malik, Jitendra and Savarese, Silvio},
  booktitle = {CVPR},
  year      = {2018}
}

@inproceedings{makoviychuk2021isaac,
  title     = {{Isaac Gym}: High performance gpu-based physics simulation for robot learning},
  author    = {Makoviychuk, Viktor and Wawrzyniak, Lukasz and Guo, Yunrong and Lu, Michelle and Storey, Kier and Macklin, Miles and Hoeller, David and Rudin, Nikita and Allshire, Arthur and Handa, Ankur and others},
  booktitle = {NeurIPS},
  year      = {2021}
}

@inproceedings{lindenberger2023lightglue,
  title     = {{LightGlue: Local Feature Matching at Light Speed}},
  author    = {Lindenberger, Philipp and Sarlin, Paul-Edouard and Pollefeys, Marc},
  booktitle = {ICCV},
  year      = {2023}
}

@article{touvron2023llama,
  title   = {{LLAMA 2: Open foundation and fine-tuned chat models}},
  author  = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal = {arXiv preprint arXiv:2307.09288},
  year    = {2023}
}

@inproceedings{kuttler2020nethack,
  title     = {The nethack learning environment},
  author    = {K{\"u}ttler, Heinrich and Nardelli, Nantas and Miller, Alexander and Raileanu, Roberta and Selvatici, Marco and Grefenstette, Edward and Rockt{\"a}schel, Tim},
  booktitle = {NeurIPS},
  year      = {2020}
}

@article{schumann2023velma,
  title   = {{VELMA}: Verbalization embodiment of llm agents for vision and language navigation in street view},
  author  = {Schumann, Raphael and Zhu, Wanrong and Feng, Weixi and Fu, Tsu-Jui and Riezler, Stefan and Wang, William Yang},
  journal = {arXiv preprint arXiv:2307.06082},
  year    = {2023}
}

@inproceedings{schumann2020generating,
  title     = {Generating landmark navigation instructions from maps as a graph-to-text problem},
  author    = {Schumann, Raphael and Riezler, Stefan},
  booktitle = {ACL},
  year      = {2020}
}

@inproceedings{chen2019touchdown,
  title     = {{TOUCHDOWN}: Natural language navigation and spatial reasoning in visual street environments},
  author    = {Chen, Howard and Suhr, Alane and Misra, Dipendra and Snavely, Noah and Artzi, Yoav},
  booktitle = {CVPR},
  year      = {2019}
}

@article{zamir2014image,
  title   = {Image geo-localization based on multiplenearest neighbor feature matching usinggeneralized graphs},
  journal = {TPAMI},
  year    = {2014}
}

@inproceedings{frome2009large,
  title     = {Large-scale privacy protection in google street view},
  author    = {Frome, Andrea and Cheung, German and Abdulkader, Ahmad and Zennaro, Marco and Wu, Bo and Bissacco, Alessandro and Adam, Hartwig and Neven, Hartmut and Vincent, Luc},
  booktitle = {ICCV},
  year      = {2009}
}

@inproceedings{li2023internet,
  title        = {Internet explorer: Targeted representation learning on the open web},
  author       = {Li, Alexander C and Brown, Ellis and Efros, Alexei A and Pathak, Deepak},
  booktitle    = {International Conference on Machine Learning},
  pages        = {19385--19406},
  year         = {2023},
  organization = {PMLR}
}

@article{wu2023vstar,
  title   = {{V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs}},
  author  = {Wu, Penghao and Xie, Saining},
  journal = {arXiv preprint arXiv:2312.14135},
  year    = {2023}
}

@article{xu2023demystifying,
  title   = {Demystifying clip data},
  author  = {Xu, Hu and Xie, Saining and Tan, Xiaoqing Ellen and Huang, Po-Yao and Howes, Russell and Sharma, Vasu and Li, Shang-Wen and Ghosh, Gargi and Zettlemoyer, Luke and Feichtenhofer, Christoph},
  journal = {arXiv preprint arXiv:2309.16671},
  year    = {2023}
}

@article{thomee2016yfcc100m,
  title   = {{YFCC100M}: The new data in multimedia research},
  author  = {Thomee, Bart and Shamma, David A and Friedland, Gerald and Elizalde, Benjamin and Ni, Karl and Poland, Douglas and Borth, Damian and Li, Li-Jia},
  journal = {Communications of the ACM},
  year    = {2016}
}

@inproceedings{sharma2018conceptual,
  title     = {{Conceptual Captions}: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
  author    = {Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  booktitle = {ACL},
  year      = {2018}
}

@inproceedings{schuhmann2022laionb,
  title     = {{LAION}-5B: An open large-scale dataset for training next generation image-text models},
  author    = {Christoph Schuhmann and
               Romain Beaumont and
               Richard Vencu and
               Cade W Gordon and
               Ross Wightman and
               Mehdi Cherti and
               Theo Coombes and
               Aarush Katta and
               Clayton Mullis and
               Mitchell Wortsman and
               Patrick Schramowski and
               Srivatsa R Kundurthy and
               Katherine Crowson and
               Ludwig Schmidt and
               Robert Kaczmarczyk and
               Jenia Jitsev},
  booktitle = {NeurIPS},
  year      = {2022}
}

@inproceedings{laurenccon2023obelisc,
  title     = {{OBELICS}: An open web-scale filtered dataset of interleaved image-text documents},
  author    = {Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Tronchon, L{\'e}o and Bekman, Stas and Singh, Amanpreet and Lozhkov, Anton and Wang, Thomas and Karamcheti, Siddharth and Rush, Alexander M and Kiela, Douwe and others},
  booktitle = {NeurIPS},
  year      = {2023}
}

@inproceedings{zhou2022detecting,
  title     = {Detecting Twenty-thousand Classes using Image-level Supervision},
  author    = {Zhou, Xingyi and Girdhar, Rohit and Joulin, Armand and Kr{\"a}henb{\"u}hl, Philipp and Misra, Ishan},
  booktitle = {ECCV},
  year      = {2022}
}

@inproceedings{ghiasi2022scaling,
  title     = {Scaling open-vocabulary image segmentation with image-level labels},
  author    = {Ghiasi, Golnaz and Gu, Xiuye and Cui, Yin and Lin, Tsung-Yi},
  booktitle = {ECCV},
  year      = {2022}
}

@inproceedings{xu2022groupvit,
  title     = {{GroupViT}: Semantic segmentation emerges from text supervision},
  author    = {Xu, Jiarui and De Mello, Shalini and Liu, Sifei and Byeon, Wonmin and Breuel, Thomas and Kautz, Jan and Wang, Xiaolong},
  booktitle = {CVPR},
  year      = {2022}
}

@article{awadalla2023openflamingo,
  title   = {{OpenFlamingo}: An open-source framework for training large autoregressive vision-language models},
  author  = {Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others},
  journal = {arXiv preprint arXiv:2308.01390},
  year    = {2023}
}

@inproceedings{rojas2022dollar,
  title     = {{The Dollar Street Dataset}: Images representing the geographic and socioeconomic diversity of the world},
  author    = {Rojas, William A Gaviria and Diamos, Sudnya and Kini, Keertan Ranjan and Kanter, David and Reddi, Vijay Janapa and Coleman, Cody},
  booktitle = {NeurIPS},
  year      = {2022}
}

@inproceedings{hong2023metagpt,
  title     = {{MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework}},
  author    = {Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Ceyao Zhang and Jinlin Wang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and JÃ¼rgen Schmidhuber},
  year      = {2023},
  booktitle = {ICLR}
}

@inproceedings{yao2023react,
  title     = {{ReAct}: Synergizing reasoning and acting in language models},
  author    = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  booktitle = {ICLR},
  year      = {2023}
}

@inproceedings{shinn2023reflexion,
  title     = {Reflexion: an autonomous agent with dynamic memory and self-reflection},
  author    = {Shinn, Noah and Labash, Beck and Gopinath, Ashwin},
  booktitle = {NeurIPS},
  year      = {2023}
}


@inproceedings{liang2023code,
  title     = {{Code as Policies}: Language model programs for embodied control},
  author    = {Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
  booktitle = {ICRA},
  year      = {2023}
}


@inproceedings{driess2023palme,
  title     = {{PaLM-E: An Embodied Multimodal Language Model}},
  author    = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
  booktitle = {ICML},
  year      = {2023}
}

@article{zhu2023ghost,
  title   = {{Ghost in the Minecraft}: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory},
  author  = {Zhu, Xizhou and Chen, Yuntao and Tian, Hao and Tao, Chenxin and Su, Weijie and Yang, Chenyu and Huang, Gao and Li, Bin and Lu, Lewei and Wang, Xiaogang and Qiao, Yu and Zhang, Zhaoxiang and Dai, Jifeng},
  journal = {arXiv preprint arXiv:2305.17144},
  year    = {2023}
}

@inproceedings{xiang2020sapien,
  title     = {{SAPIEN}: A simulated part-based interactive environment},
  author    = {Xiang, Fanbo and Qin, Yuzhe and Mo, Kaichun and Xia, Yikuan and Zhu, Hao and Liu, Fangchen and Liu, Minghua and Jiang, Hanxiao and Yuan, Yifu and Wang, He and others},
  booktitle = {CVPR},
  year      = {2020}
}

@article{berner2019dota,
  title   = {Dota 2 with large scale deep reinforcement learning},
  author  = {Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal = {arXiv preprint arXiv:1912.06680},
  year    = {2019}
}

@article{anil2023palm,
  title   = {{PaLM} 2 technical report},
  author  = {Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal = {arXiv preprint arXiv:2305.10403},
  year    = {2023}
}

@article{huang2023voxposer,
  title   = {{VoxPoser}: Composable 3d value maps for robotic manipulation with language models},
  author  = {Huang, Wenlong and Wang, Chen and Zhang, Ruohan and Li, Yunzhu and Wu, Jiajun and Fei-Fei, Li},
  journal = {arXiv preprint arXiv:2307.05973},
  year    = {2023}
}


@inproceedings{wei2022chain,
  title     = {Chain-of-thought prompting elicits reasoning in large language models},
  author    = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  booktitle = {NeurIPS},
  year      = {2022}
}

@article{qian2023communicative,
  title   = {Communicative agents for software development},
  author  = {Qian, Chen and Cong, Xin and Yang, Cheng and Chen, Weize and Su, Yusheng and Xu, Juyuan and Liu, Zhiyuan and Sun, Maosong},
  journal = {arXiv preprint arXiv:2307.07924},
  year    = {2023}
}

@article{mnih2013playing,
  title   = {Playing atari with deep reinforcement learning},
  author  = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal = {arXiv preprint arXiv:1312.5602},
  year    = {2013}
}



@inproceedings{yenamandra2023homerobot,
  title     = {{HomeRobot}: Open-Vocabulary Mobile Manipulation},
  author    = {Yenamandra, Sriram and Ramachandran, Arun and Yadav, Karmesh and Wang, Austin and Khanna, Mukul and Gervet, Theophile and Yang, Tsung-Yen and Jain, Vidhi and Clegg, Alexander William and Turner, John and others},
  booktitle = {CoRL},
  year      = {2023}
}

@article{anderson2018evaluation,
  title   = {On evaluation of embodied navigation agents},
  author  = {Anderson, Peter and Chang, Angel and Chaplot, Devendra Singh and Dosovitskiy, Alexey and Gupta, Saurabh and Koltun, Vladlen and Kosecka, Jana and Malik, Jitendra and Mottaghi, Roozbeh and Savva, Manolis and others},
  journal = {arXiv preprint arXiv:1807.06757},
  year    = {2018}
}

@inproceedings{chaplot2020object,
  title     = {Object goal navigation using goal-oriented semantic exploration},
  author    = {Chaplot, Devendra Singh and Gandhi, Dhiraj Prakashchand and Gupta, Abhinav and Salakhutdinov, Russ R},
  booktitle = {NeurIPS},
  year      = {2020}
}

@inproceedings{Matterport3D,
  title     = {{Matterport3D}: Learning from RGB-D Data in Indoor Environments},
  author    = {Chang, Angel and Dai, Angela and Funkhouser, Thomas and Halber, Maciej and Niessner, Matthias and Savva, Manolis and Song, Shuran and Zeng, Andy and Zhang, Yinda},
  booktitle = {3DV},
  year      = {2017}
}

@inproceedings{anderson2018vision,
  title     = {{Vision-and-Language Navigation}: Interpreting visually-grounded navigation instructions in real environments},
  author    = {Anderson, Peter and Wu, Qi and Teney, Damien and Bruce, Jake and Johnson, Mark and S{\"u}nderhauf, Niko and Reid, Ian and Gould, Stephen and Van Den Hengel, Anton},
  booktitle = {CVPR},
  year      = {2018}
}

@article{rt22023arxiv,
  title   = {{RT-2}: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control},
  author  = {Anthony Brohan and Noah Brown and Justice Carbajal and Yevgen Chebotar and Xi Chen and Krzysztof Choromanski and Tianli Ding and Danny Driess and Avinava Dubey and Chelsea Finn and Pete Florence and Chuyuan Fu and Montse Gonzalez Arenas and Keerthana Gopalakrishnan and Kehang Han and Karol Hausman and Alex Herzog and Jasmine Hsu and Brian Ichter and Alex Irpan and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Isabel Leal  and Lisa Lee and Tsang-Wei Edward Lee and Sergey Levine and Yao Lu and Henryk Michalewski and Igor Mordatch and Karl Pertsch and Kanishka Rao and Krista Reymann and Michael Ryoo and Grecia Salazar and Pannag Sanketi and Pierre Sermanet and Jaspiar Singh and Anikait Singh and Radu Soricut and Huong Tran and Vincent Vanhoucke and Quan Vuong and Ayzaan Wahid and Stefan Welker and Paul Wohlhart and  Jialin Wu and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Tianhe Yu and Brianna Zitkovich},
  journal = {arXiv preprint arXiv:2307.15818},
  year    = {2023}
}

@article{shao2023lmdrive,
  title   = {{LMDrive}: Closed-Loop End-to-End Driving with Large Language Models},
  author  = {Shao, Hao and Hu, Yuxuan and Wang, Letian and Waslander, Steven L and Liu, Yu and Li, Hongsheng},
  journal = {arXiv preprint arXiv:2312.07488},
  year    = {2023}
}

@inproceedings{gu2017deep,
  title     = {Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},
  author    = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  booktitle = {ICRA},
  year      = {2017}
}

@inproceedings{li2023camel,
  title     = {{CAMEL}: Communicative agents for" mind" exploration of large language model society},
  author    = {Li, Guohao and Hammoud, Hasan Abed Al Kader and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard},
  booktitle = {NeurIPS},
  year      = {2023}
}

@article{wu2023autogen,
  title   = {{AutoGen}: Enabling next-gen llm applications via multi-agent conversation framework},
  author  = {Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Zhang, Shaokun and Zhu, Erkang and Li, Beibin and Jiang, Li and Zhang, Xiaoyun and Wang, Chi},
  journal = {arXiv preprint arXiv:2308.08155},
  year    = {2023}
}

@inproceedings{wang2023gensim,
  title     = {{GenSim}: Generating Robotic Simulation Tasks via Large Language Models},
  author    = {Wang, Lirui and Ling, Yiyang and Yuan, Zhecheng and Shridhar, Mohit and Bao, Chen and Qin, Yuzhe and Wang, Bailin and Xu, Huazhe and Wang, Xiaolong},
  booktitle = {ICLR},
  year      = {2023}
}

@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6904--6913},
  year={2017}
}

@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6700--6709},
  year={2019}
}

@article{krishna2017visual,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={International journal of computer vision},
  volume={123},
  pages={32--73},
  year={2017},
  publisher={Springer}
}

@article{mangalam2023egoschema,
  title={Egoschema: A diagnostic benchmark for very long-form video language understanding},
  author={Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46212--46244},
  year={2023}
}

@inproceedings{miech2019howto100m,
  title={Howto100m: Learning a text-video embedding by watching hundred million narrated video clips},
  author={Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={2630--2640},
  year={2019}
}

@inproceedings{kamath2021mdetr,
  title={Mdetr-modulated detection for end-to-end multi-modal understanding},
  author={Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1780--1790},
  year={2021}
}
@article{peng2023kosmos,
  title={Kosmos-2: Grounding multimodal large language models to the world},
  author={Peng, Zhiliang and Wang, Wenhui and Dong, Li and Hao, Yaru and Huang, Shaohan and Ma, Shuming and Wei, Furu},
  journal={arXiv preprint arXiv:2306.14824},
  year={2023}
}


@inproceedings{das2013thousand,
  title={A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching},
  author={Das, Pradipto and Xu, Chenliang and Doell, Richard F and Corso, Jason J},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2634--2641},
  year={2013}
}

@misc{liu2023llava,
      title={Visual Instruction Tuning},
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={NeurIPS},
      year={2023},
}

@misc{zhang2024llavanextvideo,
    title={LLaVA-NeXT: A Strong Zero-shot Video Understanding Model},
    url={https://llava-vl.github.io/blog/2024-04-30-llava-next-video/},
    author={Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},
    month={April},
    year={2024}
}

@article{li2024llavaonevision,
  title={Llava-OneVision: Easy visual task transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Zhang, Peiyuan and Li, Yanwei and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}


@inproceedings{sun2019videobert,
  title={Videobert: A joint model for video and language representation learning},
  author={Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={7464--7473},
  year={2019}
}

@article{hong20233d,
  title={3d-llm: Injecting the 3d world into large language models},
  author={Hong, Yining and Zhen, Haoyu and Chen, Peihao and Zheng, Shuhong and Du, Yilun and Chen, Zhenfang and Gan, Chuang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={20482--20494},
  year={2023}
}

@inproceedings{hong20233dqa,
  title={3d concept learning and reasoning from multi-view images},
  author={Hong, Yining and Lin, Chunru and Du, Yilun and Chen, Zhenfang and Tenenbaum, Joshua B and Gan, Chuang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9202--9212},
  year={2023}
}

@misc{liu2023visualspatialreasoning,
      title={Visual Spatial Reasoning},
      author={Fangyu Liu and Guy Emerson and Nigel Collier},
      year={2023},
      eprint={2205.00363},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.00363},
}



@inproceedings{ranasinghe2024learning,
  title={Learning to localize objects improves spatial reasoning in visual-llms},
  author={Ranasinghe, Kanchana and Shukla, Satya Narayan and Poursaeed, Omid and Ryoo, Michael S and Lin, Tsung-Yu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12977--12987},
  year={2024}
}

@inproceedings{jia2021scaling,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle={International conference on machine learning},
  pages={4904--4916},
  year={2021},
  organization={PMLR}
}
@article{liu2024coarse,
  title={Coarse correspondence elicit 3d spacetime understanding in multimodal language model},
  author={Liu, Benlin and Dong, Yuhao and Wang, Yiqin and Rao, Yongming and Tang, Yansong and Ma, Wei-Chiu and Krishna, Ranjay},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}
@article{wang2022omnivl,
  title={Omnivl: One foundation model for image-language and video-language tasks},
  author={Wang, Junke and Chen, Dongdong and Wu, Zuxuan and Luo, Chong and Zhou, Luowei and Zhao, Yucheng and Xie, Yujia and Liu, Ce and Jiang, Yu-Gang and Yuan, Lu},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={5696--5710},
  year={2022}
}

@article{hsieh2023sugarcrepe,
  title={Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality},
  author={Hsieh, Cheng-Yu and Zhang, Jieyu and Ma, Zixian and Kembhavi, Aniruddha and Krishna, Ranjay},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={31096--31116},
  year={2023}
}
@inproceedings{ma2023crepe,
  title={Crepe: Can vision-language foundation models reason compositionally?},
  author={Ma, Zixian and Hong, Jerry and Gul, Mustafa Omer and Gandhi, Mona and Gao, Irena and Krishna, Ranjay},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10910--10921},
  year={2023}
}
@misc{ramakrishnan2024doesspatialcognitionemerge,
      title={Does Spatial Cognition Emerge in Frontier Models?},
      author={Santhosh Kumar Ramakrishnan and Erik Wijmans and Philipp Kraehenbuehl and Vladlen Koltun},
      year={2024},
      eprint={2410.06468},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.06468},
}

@article{chow2025physbench,
  title={PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding},
  author={Chow, Wei and Mao, Jiageng and Li, Boyi and Seita, Daniel and Guizilini, Vitor and Wang, Yue},
  journal={arXiv preprint arXiv:2501.16411},
  year={2025}
}

@misc{ehsani2024spocimitatingshortestpaths,
      title={SPOC: Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World},
      author={Kiana Ehsani and Tanmay Gupta and Rose Hendrix and Jordi Salvador and Luca Weihs and Kuo-Hao Zeng and Kunal Pratap Singh and Yejin Kim and Winson Han and Alvaro Herrasti and Ranjay Krishna and Dustin Schwenk and Eli VanderBilt and Aniruddha Kembhavi},
      year={2024},
      eprint={2312.02976},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2312.02976},
}

@article{engel2023project,
  title={Project aria: A new tool for egocentric multi-modal ai research},
  author={Engel, Jakob and Somasundaram, Kiran and Goesele, Michael and Sun, Albert and Gamino, Alexander and Turner, Andrew and Talattof, Arjang and Yuan, Arnie and Souti, Bilal and Meredith, Brighid and others},
  journal={arXiv preprint arXiv:2308.13561},
  year={2023}
}

@article{duan2023ar2,
  title={Ar2-d2: Training a robot without a robot},
  author={Duan, Jiafei and Wang, Yi Ru and Shridhar, Mohit and Fox, Dieter and Krishna, Ranjay},
  journal={arXiv preprint arXiv:2306.13818},
  year={2023}
}



@inproceedings{silwal2024we,
  title={What do we learn from a large-scale study of pre-trained visual representations in sim and real environments?},
  author={Silwal, Sneha and Yadav, Karmesh and Wu, Tingfan and Vakil, Jay and Majumdar, Arjun and Arnaud, Sergio and Chen, Claire and Berges, Vincent-Pierre and Batra, Dhruv and Rajeswaran, Aravind and others},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={17515--17521},
  year={2024},
  organization={IEEE}
}

@misc{chen2025janusprounifiedmultimodalunderstanding,
      title={Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling},
      author={Xiaokang Chen and Zhiyu Wu and Xingchao Liu and Zizheng Pan and Wen Liu and Zhenda Xie and Xingkai Yu and Chong Ruan},
      year={2025},
      eprint={2501.17811},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2501.17811},
}

@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{deitke2024molmo,
  title={Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models},
  author={Deitke, Matt and Clark, Christopher and Lee, Sangho and Tripathi, Rohun and Yang, Yue and Park, Jae Sung and Salehi, Mohammadreza and Muennighoff, Niklas and Lo, Kyle and Soldaini, Luca and others},
  journal={arXiv preprint arXiv:2409.17146},
  year={2024}
}

@misc{matas2018simtorealreinforcementlearningdeformable,
      title={Sim-to-Real Reinforcement Learning for Deformable Object Manipulation},
      author={Jan Matas and Stephen James and Andrew J. Davison},
      year={2018},
      eprint={1806.07851},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/1806.07851},
}

@article{franz2000biomimetic,
  title={Biomimetic robot navigation},
  author={Franz, Matthias O and Mallot, Hanspeter A},
  journal={Robotics and autonomous Systems},
  volume={30},
  number={1-2},
  pages={133--153},
  year={2000},
  publisher={Elsevier}
}

@article{epstein2017cognitive,
  title={The cognitive map in humans: spatial navigation and beyond},
  author={Epstein, Russell A and Patai, Eva Zita and Julian, Joshua B and Spiers, Hugo J},
  journal={Nature neuroscience},
  volume={20},
  number={11},
  pages={1504--1513},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{lin2025sim,
  title={Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids},
  author={Lin, Toru and Sachdev, Kartik and Fan, Linxi and Malik, Jitendra and Zhu, Yuke},
  journal={arXiv preprint arXiv:2502.20396},
  year={2025}
}

@misc{zeng2024poliformerscalingonpolicyrl,
      title={PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators},
      author={Kuo-Hao Zeng and Zichen Zhang and Kiana Ehsani and Rose Hendrix and Jordi Salvador and Alvaro Herrasti and Ross Girshick and Aniruddha Kembhavi and Luca Weihs},
      year={2024},
      eprint={2406.20083},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2406.20083},
}

@misc{mishra2024syncdrtrainingcross,
      title={SynCDR : Training Cross Domain Retrieval Models with Synthetic Data},
      author={Samarth Mishra and Carlos D. Castillo and Hongcheng Wang and Kate Saenko and Venkatesh Saligrama},
      year={2024},
      eprint={2401.00420},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2401.00420},
}

@misc{fu2024videommefirstevercomprehensiveevaluation,
      title={Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis},
      author={Chaoyou Fu and Yuhan Dai and Yongdong Luo and Lei Li and Shuhuai Ren and Renrui Zhang and Zihan Wang and Chenyu Zhou and Yunhang Shen and Mengdan Zhang and Peixian Chen and Yanwei Li and Shaohui Lin and Sirui Zhao and Ke Li and Tong Xu and Xiawu Zheng and Enhong Chen and Rongrong Ji and Xing Sun},
      year={2024},
      eprint={2405.21075},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.21075},
}

@inproceedings{grauman2024ego,
  title={Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives},
  author={Grauman, Kristen and Westbury, Andrew and Torresani, Lorenzo and Kitani, Kris and Malik, Jitendra and Afouras, Triantafyllos and Ashutosh, Kumar and Baiyya, Vijay and Bansal, Siddhant and Boote, Bikram and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19383--19400},
  year={2024}
}

@misc{grauman2022ego4dworld3000hours,
      title={Ego4D: Around the World in 3,000 Hours of Egocentric Video},
      author={Kristen Grauman and Andrew Westbury and Eugene Byrne and Zachary Chavis and Antonino Furnari and Rohit Girdhar and Jackson Hamburger and Hao Jiang and Miao Liu and Xingyu Liu and Miguel Martin and Tushar Nagarajan and Ilija Radosavovic and Santhosh Kumar Ramakrishnan and Fiona Ryan and Jayant Sharma and Michael Wray and Mengmeng Xu and Eric Zhongcong Xu and Chen Zhao and Siddhant Bansal and Dhruv Batra and Vincent Cartillier and Sean Crane and Tien Do and Morrie Doulaty and Akshay Erapalli and Christoph Feichtenhofer and Adriano Fragomeni and Qichen Fu and Abrham Gebreselasie and Cristina Gonzalez and James Hillis and Xuhua Huang and Yifei Huang and Wenqi Jia and Weslie Khoo and Jachym Kolar and Satwik Kottur and Anurag Kumar and Federico Landini and Chao Li and Yanghao Li and Zhenqiang Li and Karttikeya Mangalam and Raghava Modhugu and Jonathan Munro and Tullie Murrell and Takumi Nishiyasu and Will Price and Paola Ruiz Puentes and Merey Ramazanova and Leda Sari and Kiran Somasundaram and Audrey Southerland and Yusuke Sugano and Ruijie Tao and Minh Vo and Yuchen Wang and Xindi Wu and Takuma Yagi and Ziwei Zhao and Yunyi Zhu and Pablo Arbelaez and David Crandall and Dima Damen and Giovanni Maria Farinella and Christian Fuegen and Bernard Ghanem and Vamsi Krishna Ithapu and C. V. Jawahar and Hanbyul Joo and Kris Kitani and Haizhou Li and Richard Newcombe and Aude Oliva and Hyun Soo Park and James M. Rehg and Yoichi Sato and Jianbo Shi and Mike Zheng Shou and Antonio Torralba and Lorenzo Torresani and Mingfei Yan and Jitendra Malik},
      year={2022},
      eprint={2110.07058},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2110.07058},
}

 @inproceedings{OpenEQA2023,
        title         = {OpenEQA: Embodied Question Answering in the Era of Foundation Models},
        booktitle     = {Conference on Computer Vision and Pattern Recognition (CVPR)},
        author        = {Majumdar, Arjun and Ajay, Anurag and Zhang, Xiaohan and Putta, Pranav and Yenamandra, Sriram and Henaff, Mikael and Silwal, Sneha and Mcvay, Paul and Maksymets, Oleksandr and Arnaud, Sergio and Yadav, Karmesh and Li, Qiyang and Newman, Ben and Sharma, Mohit and Berges, Vincent and Zhang, Shiqi and Agrawal, Pulkit and Bisk, Yonatan and Batra, Dhruv and Kalakrishnan, Mrinal and Meier, Franziska and Paxton, Chris and Sax, Sasha and Rajeswaran, Aravind},
        year          = {2024},
    }



@misc{bai2023qwenvlversatilevisionlanguagemodel,
      title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
      author={Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou},
      year={2023},
      eprint={2308.12966},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2308.12966},
}

@misc{singh2022flavafoundationallanguagevision,
      title={FLAVA: A Foundational Language And Vision Alignment Model},
      author={Amanpreet Singh and Ronghang Hu and Vedanuj Goswami and Guillaume Couairon and Wojciech Galuba and Marcus Rohrbach and Douwe Kiela},
      year={2022},
      eprint={2112.04482},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.04482},
}

@article{fu2021violet,
  title={Violet: End-to-end video-language transformers with masked visual-token modeling},
  author={Fu, Tsu-Jui and Li, Linjie and Gan, Zhe and Lin, Kevin and Wang, William Yang and Wang, Lijuan and Liu, Zicheng},
  journal={arXiv preprint arXiv:2111.12681},
  year={2021}
}



@article{wang2022internvideo,
  title={Internvideo: General video foundation models via generative and discriminative learning},
  author={Wang, Yi and Li, Kunchang and Li, Yizhuo and He, Yinan and Huang, Bingkun and Zhao, Zhiyu and Zhang, Hongjie and Xu, Jilan and Liu, Yi and Wang, Zun and others},
  journal={arXiv preprint arXiv:2212.03191},
  year={2022}
}



@inproceedings{minderer2022simple,
  title={Simple open-vocabulary object detection},
  author={Minderer, Matthias and Gritsenko, Alexey and Stone, Austin and Neumann, Maxim and Weissenborn, Dirk and Dosovitskiy, Alexey and Mahendran, Aravindh and Arnab, Anurag and Dehghani, Mostafa and Shen, Zhuoran and others},
  booktitle={European conference on computer vision},
  pages={728--755},
  year={2022},
  organization={Springer}
}

@inproceedings{fu2024blink,
  title={Blink: Multimodal large language models can see but not perceive},
  author={Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},
  booktitle={European Conference on Computer Vision},
  pages={148--166},
  year={2024},
  organization={Springer}
}


@article{tong2025cambrian,
  title={Cambrian-1: A fully open, vision-centric exploration of multimodal llms},
  author={Tong, Peter and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and IYER, Adithya Jairam Vedagiri and Akula, Sai Charitha and Yang, Shusheng and Yang, Jihan and Middepogu, Manoj and Wang, Ziteng and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={87310--87356},
  year={2025}
}


@article{cheng2025spatialrgpt,
  title={Spatialrgpt: Grounded spatial reasoning in vision-language models},
  author={Cheng, An-Chieh and Yin, Hongxu and Fu, Yang and Guo, Qiushan and Yang, Ruihan and Kautz, Jan and Wang, Xiaolong and Liu, Sifei},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={135062--135093},
  year={2025}
}

@inproceedings{chen2024spatialvlm,
  title={Spatialvlm: Endowing vision-language models with spatial reasoning capabilities},
  author={Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brain and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14455--14465},
  year={2024}
}

@article{ray2024sat,
  title={SAT: Spatial Aptitude Training for Multimodal Language Models},
  author={Ray, Arijit and Duan, Jiafei and Tan, Reuben and Bashkirova, Dina and Hendrix, Rose and Ehsani, Kiana and Kembhavi, Aniruddha and Plummer, Bryan A and Krishna, Ranjay and Zeng, Kuo-Hao and others},
  journal={arXiv preprint arXiv:2412.07755},
  year={2024}
}




@article{zhang2023omni,
  title   = {{OMNI}: Open-endedness via Models of human Notions of Interestingness},
  author  = {Zhang, Jenny and Lehman, Joel and Stanley, Kenneth and Clune, Jeff},
  journal = {arXiv preprint arXiv:2306.01711},
  year    = {2023}
}

@article{lin2023text2motion,
  title   = {{Text2Motion}: From natural language instructions to feasible plans},
  author  = {Lin, Kevin and Agia, Christopher and Migimatsu, Toki and Pavone, Marco and Bohg, Jeannette},
  journal = {arXiv preprint arXiv:2303.12153},
  year    = {2023}
}

@inproceedings{liu2023reflect,
  title     = {{REFLECT}: Summarizing robot experiences for failure explanation and correction},
  author    = {Liu, Zeyi and Bahety, Arpit and Song, Shuran},
  booktitle = {CoRL},
  year      = {2023}
}

@article{yang2024think,
  title   = {{Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces}},
  author  = {Yang, Jihan and Yang, Shusheng and Gupta, Anjali W. and Han, Rilyn and Fei-Fei, Li and Xie, Saining},
  year    = {2024},
  journal = {arXiv preprint arXiv:2412.14171}
}

@inproceedings{deitke2023objaverse,
  title     = {{Objaverse: A Universe of Annotated 3D Objects}},
  author    = {Deitke, Matt and Schwenk, Dustin and Salvador, Jordi and Weihs, Luca and Michel, Oscar and VanderBilt, Eli and Schmidt, Ludwig and Ehsani, Kiana and Kembhavi, Aniruddha and Farhadi, Ali},
  booktitle = {CVPR},
  year      = {2023}
}

@inproceedings{deitke2022procthor,
  title={{ProcTHOR: Large-Scale Embodied AI Using Procedural Generation}},
  author={Deitke, Matt and VanderBilt, Eli and Herrasti, Alvaro and Weihs, Luca and Ehsani, Kiana and Salvador, Jordi and Han, Winson and Kolve, Eric and Kembhavi, Aniruddha and Mottaghi, Roozbeh},
  booktitle={NeurIPS},
  year={2022}
}

@article{yuan2024robopoint,
  title={RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics},
  author={Yuan, Wentao and Duan, Jiafei and Blukis, Valts and Pumacay, Wilbert and Krishna, Ranjay and Murali, Adithyavairavan and Mousavian, Arsalan and Fox, Dieter},
  journal={arXiv preprint arXiv:2406.10721},
  year={2024}
}


@article{kolve2017ai2,
  title={{AI2-THOR: An Interactive 3D Environment for Visual AI}},
  author={Eric Kolve and Roozbeh Mottaghi and Winson Han and Eli VanderBilt and Luca Weihs and Alvaro Herrasti and Matt Deitke and Kiana Ehsani and Daniel Gordon and Yuke Zhu and Aniruddha Kembhavi and Abhinav Kumar Gupta and Ali Farhadi},
  journal={arXiv preprint arXiv:1712.05474},
  year={2017}
}

@article{zhang2024video,
  title={{Video Instruction Tuning with Synthetic Data}},
  author={Zhang, Yuanhan and Wu, Jinming and Li, Wei and Li, Bo and Ma, Zejun and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2410.02713},
  year={2024}
}

@article{yang2024qwen2,
  title={{Qwen2 Technical Report}},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@inproceedings{zhai2023sigmoid,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={CVPR},
  pages={11975--11986},
  year={2023}
}


@article{tobin2017domain,
  title={Domain randomization for transferring deep neural networks from simulation to the real world},
  author={Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  journal={IROS},
  year={2017},
}

@article{tzeng2020adapting,
  title={Adapting deep visuomotor representations with weak pairwise constraints},
  author={Tzeng, Eric and Devin, Coline and Hoffman, Judy and Finn, Chelsea and Abbeel, Pieter and Levine, Sergey and Saenko, Kate and Darrell, Trevor},
  journal={Algorithmic Foundations of Robotics XII},
  year={2020},
}

@article{liu2020stereogan,
  title={Stereogan: Bridging synthetic-to-real domain gap by joint optimization of domain translation and stereo matching},
  author={Liu, Rui and Yang, Chengxi and Sun, Wenxiu and Wang, Xiaogang and Li, Hongsheng},
  journal={CVPR},
  year={2020}
}
@article{brown2025shortcuts,
    author = {Brown, Ellis and Yang, Jihan and Yang, Shusheng and Fergus, Rob and Xie, Saining},
    title = {Benchmark Designers Should ``Train on the Test Set'' to Expose Exploitable Non-Visual Shortcuts},
    journal = {arXiv preprint},
    year = {2025}
}
@inproceedings{chatterjee2024revision,
  title={REVISION: Rendering tools enable spatial fidelity in vision-language models},
  author={Chatterjee, Agneet and Luo, Yiran and Gokhale, Tejas and Yang, Yezhou and Baral, Chitta},
  booktitle={ECCV},
  year={2024},
}

@inproceedings{cascante2023going,
  title={Going beyond nouns with vision \& language models using synthetic data},
  author={Cascante-Bonilla, Paola and Shehada, Khaled and Smith, James Seale and Doveh, Sivan and Kim, Donghyun and Panda, Rameswar and Varol, Gul and Oliva, Aude and Ordonez, Vicente and Feris, Rogerio and Karlinsky, Leonid},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{zhang2024mme,
  title={MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?},
  author={Zhang, Yi-Fan and Zhang, Huanyu and Tian, Haochen and Fu, Chaoyou and Zhang, Shuangqing and Wu, Junfei and Li, Feng and Wang, Kun and Wen, Qingsong and Zhang, Zhang and others},
  booktitle={ICLR},
  year={2025}
}

@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}
@article{comanici2025gemini,
  title={Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities},
  author={Comanici, Gheorghe and Bieber, Eric and Schaekermann, Mike and Pasupat, Ice and Sachdeva, Noveen and Dhillon, Inderjit and Blistein, Marcel and Ram, Ori and Zhang, Dan and Rosen, Evan and others},
  journal={arXiv preprint arXiv:2507.06261},
  year={2025}
}
@article{chen2024expanding,
  title={Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}
@article{Qwen2.5-VL,
  title={Qwen2.5-VL Technical Report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}