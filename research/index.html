<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> research | Ellis Brown </title> <meta name="author" content="Ellis L. Brown II"> <meta name="description" content=""> <meta name="keywords" content="self-supervised learning, representation learning, multimodal, vision-language, generalization, curiosity, deep learning, computer vision, machine learning, artificial intelligence"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%B4&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ellisbrown.github.io//research/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Ellis Brown </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/research/">research <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">research</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <p></p> <p>I’m interested in self-supervised learning, representation learning, curiosity-based exploration, and leveraging internet-scale models and data. I am keen to draw inspiration from intelligence in humans and nature—especially as a goal-post rather than a blueprint. My long-term goal is to develop intelligent agents that can <em>generalize</em> and <em>continually adapt</em> as robustly and efficiently as humans do, allowing them to be <em>safely</em> deployed in the real world.</p> <h3>Publications</h3> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/virl.webp" sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/virl.webp" class="preview z-depth-1 rounded" width="100%" height="auto" alt="virl.webp" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yang2024virl" class="col-sm-8"> <div class="title">V-IRL: Grounding Virtual Intelligence in Real Life</div> <div class="author"> <a href="https://jihanyang.github.io/" rel="external nofollow noopener" target="_blank">Jihan Yang</a> ,  Runyu Ding ,  <em>Ellis Brown</em> ,  Xiaojuan Qi ,  and  <a href="https://www.sainingxie.com/" rel="external nofollow noopener" target="_blank">Saining Xie</a> </div> <div class="periodical"> <em>arXiv preprint arXiv:2402.03310</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.03310" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://virl-platform.github.io/static/V-IRL.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/VIRL-Platform/VIRL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://virl-platform.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as ﬂexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yang2024virl</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Jihan and Ding, Runyu and Brown, Ellis and Qi, Xiaojuan and Xie, Saining}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{V-IRL: Grounding Virtual Intelligence in Real Life}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2402.03310}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 abbr"> <abbr class="badge">Master’s Thesis</abbr> </div> <div id="brown2023online" class="col-sm-8"> <div class="title">Online Representation Learning on the Open Web</div> <div class="author"> <em>Ellis Brown</em> </div> <div class="periodical"> <em>Carnegie Mellon University</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ellisbrown.github.io/assets/pdf/papers/2023_CMU_Masters_Thesis.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="https://github.com/ellisbrown/cmu-masters-thesis" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://ellisbrown.github.io/assets/pdf/presentations/2023_CMU_Masters_Defense.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@mastersthesis</span><span class="p">{</span><span class="nl">brown2023online</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Brown, Ellis}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Online Representation Learning on the Open Web}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{Carnegie Mellon University}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://youtu.be/haT48f2TPxs" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"></iframe> </figure> </div> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/diffusion_classifier-800.webp 800w,/assets/img/publication_preview/diffusion_classifier-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/diffusion_classifier.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="diffusion_classifier.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2023diffusion" class="col-sm-8"> <div class="title">Your Diffusion Model is Secretly a Zero-Shot Classifier</div> <div class="author"> <a href="https://alexanderli.com" rel="external nofollow noopener" target="_blank">Alexander C. Li</a> ,  <a href="https://mihirp1998.github.io/" rel="external nofollow noopener" target="_blank">Mihir Prabhudesai</a> ,  <a href="https://shivamduggal4.github.io/" rel="external nofollow noopener" target="_blank">Shivam Duggal</a> ,  <em>Ellis Brown</em> ,  and  <a href="https://www.cs.cmu.edu/~dpathak" rel="external nofollow noopener" target="_blank">Deepak Pathak</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2303.16203" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://diffusion-classifier.github.io/static/docs/DiffusionClassifier.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://diffusion-classifier.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. <br><br> In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification, which we call Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, our diffusion-based approach has significantly stronger multimodal compositional reasoning ability than competing discriminative approaches. <br><br> Finally, we use Diffusion Classifier to extract standard classifiers from class-conditional diffusion models trained on ImageNet. Our models achieve strong classification performance using only weak augmentations and exhibit qualitatively better "effective robustness" to distribution shift. Overall, our results are a step toward using generative over discriminative models for downstream tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2023diffusion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Your Diffusion Model is Secretly a Zero-Shot Classifier}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Alexander C. and Prabhudesai, Mihir and Duggal, Shivam and Brown, Ellis and Pathak, Deepak}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2206-2217}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/internet_explorer-800.webp 800w,/assets/img/publication_preview/internet_explorer-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/internet_explorer.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="internet_explorer.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2023internet" class="col-sm-8"> <div class="title">Internet Explorer: Targeted Representation Learning on the Open Web</div> <div class="author"> <a href="https://alexanderli.com" rel="external nofollow noopener" target="_blank">Alexander C. Li*</a> ,  <em>Ellis Brown*</em> ,  <a href="https://www.cs.berkeley.edu/~efros/" rel="external nofollow noopener" target="_blank">Alexei A. Efros</a> ,  and  <a href="https://www.cs.cmu.edu/~dpathak" rel="external nofollow noopener" target="_blank">Deepak Pathak</a> </div> <div class="periodical"> <em>In International Conference on Machine Learning</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2302.14051" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://internet-explorer-ssl.github.io/static/docs/InternetExplorer.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://internet-explorer-ssl.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Vision models heavily rely on fine-tuning general-purpose models pre-trained on large, static datasets. These general-purpose models only understand knowledge within their pre-training datasets, which are tiny, out-of-date snapshots of the Internet—where billions of images are uploaded each day. We suggest an alternate approach: rather than hoping our static datasets transfer to our desired tasks after large-scale pre-training, we propose dynamically utilizing the Internet to quickly train a small-scale model that does extremely well on the task at hand. Our approach, called Internet Explorer, explores the web in a self-supervised manner to progressively find relevant examples that improve performance on a desired target dataset. It cycles between searching for images on the Internet with text queries, self-supervised training on downloaded images, determining which images were useful, and prioritizing what to search for next. We evaluate Internet Explorer across several datasets and show that it outperforms or matches CLIP oracle performance by using just a single GPU desktop to actively query the Internet for 30–40 hours.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2023internet</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Internet Explorer: Targeted Representation Learning on the Open Web}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li*, Alexander C. and Brown*, Ellis and Efros, Alexei A. and Pathak, Deepak}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2022_eccv_sslwin-800.webp 800w,/assets/img/publication_preview/2022_eccv_sslwin-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/2022_eccv_sslwin.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2022_eccv_sslwin.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2022internetcuriosity" class="col-sm-8"> <div class="title">Internet Curiosity: Directed Unsupervised Learning on Uncurated Internet Data</div> <div class="author"> <a href="https://alexanderli.com" rel="external nofollow noopener" target="_blank">Alexander C. Li*</a> ,  <em>Ellis Brown*</em> ,  <a href="https://www.cs.berkeley.edu/~efros/" rel="external nofollow noopener" target="_blank">Alexei A. Efros</a> ,  and  <a href="https://www.cs.cmu.edu/~dpathak" rel="external nofollow noopener" target="_blank">Deepak Pathak</a> </div> <div class="periodical"> <em>In European Conference on Computer Vision Workshop on “Self Supervised Learning: What is Next?”</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-031-25069-9_7" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/presentations/2022_ECCV_InternetCuriosity_Poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>We show that a curiosity-driven computer vision algorithm can learn to efficiently query Internet text-to-image search engines for images that improve the model’s performance on a specified dataset. In contrast to typical self-supervised computer vision algorithms, which learn from static datasets, our model actively expands its training set with the most relevant images. First, we calculate the image-level curiosity reward as the negative distance of an image’s representation to its nearest neighbor in the targeted dataset. This reward is easily estimated using only unlabeled data from the targeted dataset, and can be aggregated into a query-level reward that effectively identifies useful queries. Second, we use text embedding similarity scores to propagate observed curiosity rewards to untried text queries. This efficiently identifies relevant semantic clusters without any need for class labels or label names from the targeted dataset. Our method significantly outperforms models that require 1-2 orders of magnitude more compute and data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2022internetcuriosity</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Internet Curiosity: Directed Unsupervised Learning on Uncurated Internet Data}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li*, Alexander C. and Brown*, Ellis and Efros, Alexei A. and Pathak, Deepak}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision Workshop on ``Self Supervised Learning: What is Next?''}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Tel Aviv, Israel}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2018_stts-800.webp 800w,/assets/img/publication_preview/2018_stts-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/2018_stts.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2018_stts.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="brown2018stts" class="col-sm-8"> <div class="title">An Architecture for Spatiotemporal Template-Based Search</div> <div class="author"> <em>Ellis Brown</em> ,  Soobeen Park ,  Noel Wardord ,  Adriane Seiffert ,  Kazuhiko Kawamura ,  Joseph Lappin ,  and  Maithilee Kunda </div> <div class="periodical"> <em>Advances in Cognitive Systems</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://cogsys.org/journal/volume6/article-6-8.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Visual search for a spatiotemporal target occurs frequently in human experience—from military or aviation staff monitoring complex displays of multiple moving objects to daycare teachers monitoring a group of children on a playground for risky behaviors. In spatiotemporal search, unlike more traditional visual search tasks, the target cannot be identified from a single frame of visual experience; as the target is a spatiotemporal pattern that unfolds over time, detection of the target must also integrate information over time. We propose a new computational cognitive architecture used to model and understand human visual attention in the specific context of visual search for a spatiotemporal target. Results from a previous human participant study found that humans show interesting attentional capacity limitations in this type of search task. Our architecture, called the SpatioTemporal Template-based Search (STTS) architecture, solves the same search task from the study using a wide variety of parameterized mod els that each represent a different cognitive theory of visual attention from the psychological literature. We present results from initial computational experiments using STTS as a first step towards understanding the computational nature of attentional bottlenecks in this type of search task, and we discuss how continued STTS experiments will help determine which theoretical models best explain the capacity limitations shown by humans. We expect that results from this research will help refine the design of visual information displays to help human operators perform difficult, real-world monitoring tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">brown2018stts</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An Architecture for Spatiotemporal Template-Based Search}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Brown, Ellis and Park, Soobeen and Wardord, Noel and Seiffert, Adriane and Kawamura, Kazuhiko and Lappin, Joseph and Kunda, Maithilee}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Cognitive Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{101--118}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3 abbr"> <abbr class="badge">ACS-18</abbr> </div> <div id="brown2018spatiotemporal" class="col-sm-8"> <div class="title">SpatioTemporal Template-based Search: An Architecture for Spatiotemporal Template-Based Search</div> <div class="author"> <em>Ellis Brown</em> ,  Soobeen Park ,  Noel Warford ,  Adriane Seiffert ,  Kazuhiko Kawamura ,  Joe Lappin ,  and  Maithilee Kunda </div> <div class="periodical"> <em>In Proceedings of the 6th Conference on Advances in Cognitive Systems</em> , Aug 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="https://my.vanderbilt.edu/aivaslab/files/2022/03/Brown-et-al-2018-SpatioTemporal-Template-based-Search-conference-paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://docs.google.com/presentation/d/1YmzagDd5uLdEq42bkXCAX-vy0kdYvxeQp9jIYhFo-z8/present" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </div> </div> </div> </li> </ol> <br><br><br> <h3>Talks</h3> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/julia_firstorder-800.webp 800w,/assets/img/publication_preview/julia_firstorder-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/julia_firstorder.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="julia_firstorder.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="brown2021separableopt" class="col-sm-8"> <div class="title">Linearly Constrained Separable Optimization</div> <div class="author"> <em>Ellis Brown</em> ,  Nicholas Moehle ,  and  Mykel J. Kochenderfer </div> <div class="periodical"> <em>In JuliaCon 2021 JuMP Track</em> , Jul 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="/blog/2021/julia-first-order/" class="btn btn-sm z-depth-0" role="button">Blog</a> </div> <div class="abstract hidden"> <p>Many optimization problems involve minimizing a sum of univariate functions, each with a different variable, subject to coupling constraints. We present <a href="https://juliafirstorder.github.io/PiecewiseQuadratics.jl/stable/" rel="external nofollow noopener" target="_blank">PiecewiseQuadratics.jl</a> and <a href="https://juliafirstorder.github.io/SeparableOptimization.jl/stable/" rel="external nofollow noopener" target="_blank">SeparableOptimization.jl</a>, two Julia packages for solving such problems when these univariate functions in the objective are piecewise-quadratic.</p> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://www.youtube.com/watch?v=9iXWtqm60sQ" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"></iframe> </figure> </div> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 abbr"> <abbr class="badge">AISES-19</abbr> </div> <div id="brown2019aisesuncertainty" class="col-sm-8"> <div class="title">Modeling Uncertainty in Bayesian Neural Networks with Dropout: The effect of weight prior and network architecture selection</div> <div class="author"> <em>Ellis Brown*</em> ,  Melanie Manko* ,  and  Ethan Matlin* </div> <div class="periodical"> <em>In American Indian Science and Engineering Society National Conference</em> , Oct 2019 </div> <div class="periodical"> </div> <div class="award"> 🎖️ <span style="color: red; font-style: italic;">Third Place, Graduate Student Research Competition</span> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/presentations/2019_bnn_uncertainty_aises_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>While neural networks are quite successful at making predictions, these predictions are usually point estimates lacking any notion of uncertainty. However, when fed data very different from its training data, it is useful for a neural network to realize that its predictions could very well be wrong and encode that information through uncertainty bands around its point estimate prediction. Bayesian Neural Networks trained with Dropout are a natural way of modeling this uncertainty with theoretical foundations relating them to Variational Inference approximating Gaussian Process posteriors. In this paper, we investigate the effects of weight prior selection and network architecture on uncertainty estimates derived from Dropout Bayesian Neural Networks.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 abbr"> <abbr class="badge">AISES-17</abbr> </div> <div id="brown2017infosalience" class="col-sm-8"> <div class="title">Computational Cognitive Systems to Model Information Salience</div> <div class="author"> <em>Ellis Brown</em> ,  Adriane Seiffert ,  Noel Warford ,  Soobeen Park ,  and  Maithilee Kunda </div> <div class="periodical"> <em>In American Indian Science and Engineering Society National Conference</em> , Sep 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://docs.google.com/presentation/d/e/2PACX-1vTZMsXTsUARrHq0TMRYhMBA3yaHOUI8OnS0GmsczXkZEDnjPtLuTj-py7MAGPmCs5FmpAkHTILFdTdr/pub?start=false&amp;loop=false&amp;delayms=3000" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> <a href="https://my.vanderbilt.edu/aivaslab/2017/09/ellis-brown-gives-talk-at-aises-2017-national-conference/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>This project seeks to model human information salience—in this case, how much a person will notice a piece of visual information—using an approach from artificial intelligence that involves building computational cognitive systems models of human performance on certain tasks. Too much visual information can be overwhelming, making it difficult for people to discern the important parts. This can be detrimental in many situations where visual attention is crucial, such as air traffic control systems, military information displays, or TSA screening stations. Results from a previous human participant study found that humans show interesting limitations in attentional capacity when asked to monitor a complex moving display. We create artificial computational agents based on various cognitive models of visual attention to solve the same visual information salience task and run computational experiments to measure cognitive performance. In follow-up work, we will compare the performance of our models to the human data to determine which models best explain the capacity limitations shown by people.</p> </div> </div> </div> </li></ol> <br><br><br> <h3>Reports</h3> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 abbr"> <abbr class="badge">CMU 16-824</abbr> </div> <div id="report:shek2022reprexplore" class="col-sm-8"> <div class="title">Self-Supervised Representation Learning via Curiosity-Driven Exploration</div> <div class="author"> Alvin Shek ,  <em>Ellis Brown</em> ,  Nilay Pande ,  and  David Noursi </div> <div class="periodical"> May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ellisbrown.notion.site/Self-Supervised-Representation-Learning-via-Curiosity-Driven-Exploration-0cff22673f244cfeb6a6c3c8a0d0af37" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>“The performance of machine learning methods is heavily dependent on the choice of data representation” — Bengio et al, 2012.<br> As machine learning continues to be applied to more complex and important tasks, this dependence on the data representation will only increase. While current machine learning methods are bottlenecked by representation quality, current methods for learning representations are bottlenecked on the dataset size. But this process of creating large static datasets, as is the mainstream practice, is expensive, time consuming, and heavily prone to human bias.<br> Machine learning practitioners have increasingly been focusing on paradigms such as unsupervised and self-supervised learning to help alleviate the expense of supervision in working with bigger datasets; however, these methods still suffer from the issues of static datasets. One promising approach to learn good representations without a fixed datasets is by directly interacting with the environment. The visual state space of real environments/simulators can be quite huge and intractable to explore fully. Hence, in this project, we investigate intelligent curiosity driven exploration strategies to learn good representations from a simulator using self supervised learning objectives. We discuss the effectiveness of different strategies, issues and future directions of research in this field.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 abbr"> <abbr class="badge">CMU 16-811</abbr> </div> <div id="report:brown2021interp" class="col-sm-8"> <div class="title">Scaling Interpretable Reinforcement Learning via Decision Trees to Minecraft</div> <div class="author"> <em>Ellis Brown</em> ,  and  Aaron M. Roth </div> <div class="periodical"> Dec 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/papers/2021_interpretable_minerl.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Deep reinforcement learning is a powerful tool for learning complex control tasks; however, neural networks are notoriously “black boxes” and lack many properties desirable of autonymous systems deployed in safety critical environments. In this project, we focus on methods that result in a final control policy specified via a decision tree—which is thus interpretable and verifiable. We build upon a prior method, VIPER, that first learns a high-performing “expert” policy via any standard Deep RL technique, and then distills the expert policy into a decision tree. Our method, called MSVIPER, is specifically designed to scale to complex environements that greatly benefit from (or require) curriculum learning to be solved; we leverage the structure in the currculum stages to enable more efficient learning and a smaller (and thus more interpretable) decision tree. To demonstrate the ability of our method to succeed in complex environments, we apply it to Minecraft—a challenging open-world environment. We highlight that our method is amennable to post-training verification and modification or improvement.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 abbr"> <abbr class="badge">Stanford CS 361</abbr> </div> <div id="report:brown2020seclending" class="col-sm-8"> <div class="title">Securities Lending Policy Optimization</div> <div class="author"> <em>Ellis Brown</em> </div> <div class="periodical"> Jun 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/papers/2020_seclending_policy_opt.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper presents a method to determine an optimal policy for the lending of securities by large institutions in the securities finance market as a final project for the Stanford University AA222 Engineering Design Optimization class. The securities lending process is formulated as a Markov decision process in which the lender decides whether to accept or reject incoming offers from borrowers. This formulation allows for a policy that maximizes the expected return with each decision to be derived using dynamic programming. The framework presented is easily extensible through the creation of more realistic models of the dynamics of the securities lending market.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3 abbr"> <abbr class="badge">Columbia CS E6699</abbr> </div> <div id="report:brown2019uncertainty" class="col-sm-8"> <div class="title">Modeling Uncertainty in Bayesian Neural Networks with Dropout</div> <div class="author"> <em>Ellis Brown*</em> ,  Melanie Manko* ,  and  Ethan Matlin* </div> <div class="periodical"> May 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/papers/2019_bnn_uncertainty.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/presentations/2019_bnn_uncertainty_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>While neural networks are quite successful at making predictions, these predictions are usually point estimates lacking any notion of uncertainty. However, when fed data very different from its training data, it is useful for a neural network to realize that its predictions could very well be wrong and encode that information through uncertainty bands around its point estimate prediction. Bayesian Neural Networks trained with Dropout are a natural way of modeling this uncertainty with theoretical foundations relating them to Variational Inference approximating Gaussian Process posteriors. In this paper, we investigate the effects of weight prior selection and network architecture on uncertainty estimates derived from Dropout Bayesian Neural Networks.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Ellis L. Brown II. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: June 13, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-00N9EWF2WT"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-00N9EWF2WT");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>