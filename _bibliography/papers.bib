---
---

@string{aps = {American Physical Society,}}

@mastersthesis{brown2023online,
    author       = {Ellis Brown},
    bibtex_show={true},
    title        = {Online Representation Learning on the Open Web},
    school       = {Carnegie Mellon University},
    year         = 2023,
    abbr         = {Master's Thesis},
    pdf          = {https://ellisbrown.github.io/assets/pdf/papers/2023_CMU_Masters_Thesis.pdf},
    slides       = {https://ellisbrown.github.io/assets/pdf/presentations/2023_CMU_Masters_Defense.pdf},
    video        = {https://youtu.be/haT48f2TPxs},
    code         = {https://github.com/ellisbrown/cmu-masters-thesis},
}


@article{li2023diffusion,
    abbr={preprint},
    bibtex_show={true},
    title={Your Diffusion Model is Secretly a Zero-Shot Classifier}, 
    author={Alexander C. Li and Mihir Prabhudesai and Shivam Duggal and Ellis Brown and Deepak Pathak},
    year={2023},
    journal={arXiv:cs.LG},
    arxiv={2303.16203},
    eprint={2303.16203},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    website={https://diffusion-classifier.github.io/},
    pdf={https://diffusion-classifier.github.io/static/docs/DiffusionClassifier.pdf},
    preview={diffusion_classifier.jpg},
    abstract={The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation.
    <br><br>
    In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification, which we call Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, we find that our diffusion-based approach has stronger multimodal relational reasoning abilities than competing discriminative approaches.
    <br><br>
    Finally, we use Diffusion Classifier to extract standard classifiers from class-conditional diffusion models trained on ImageNet. Even though these diffusion models are trained with weak augmentations and no regularization, we find that they approach the performance of SOTA discriminative ImageNet classifiers. Overall, our strong generalization and robustness results represent an encouraging step toward using generative over discriminative models for downstream tasks.},
    selected={true},
}

@inproceedings{li2023internet,
    abbr={preprint},
    bibtex_show={true},
    title={Internet Explorer: Targeted Representation Learning on the Open Web},
    author={Li*, Alexander C. and Brown*, Ellis and Efros, Alexei A., and Pathak, Deepak},
    year={2023},
    booktitle={International Conference on Machine Learning},
    organization={PMLR},
    selected={true},
    arxiv={2302.14051},
    website={https://internet-explorer-ssl.github.io/},
    pdf={https://internet-explorer-ssl.github.io/static/docs/InternetExplorer.pdf},
    preview={internet_explorer.gif},
    abstract={Vision models heavily rely on fine-tuning general-purpose models pre-trained on large, static datasets. These general-purpose models only understand knowledge within their pre-training datasets, which are tiny, out-of-date snapshots of the Internet—where billions of images are uploaded each day. We suggest an alternate approach: rather than hoping our static datasets transfer to our desired tasks after large-scale pre-training, we propose dynamically utilizing the Internet to quickly train a small-scale model that does extremely well on the task at hand. Our approach, called Internet Explorer, explores the web in a self-supervised manner to progressively find relevant examples that improve performance on a desired target dataset. It cycles between searching for images on the Internet with text queries, self-supervised training on downloaded images, determining which images were useful, and prioritizing what to search for next. We evaluate Internet Explorer across several datasets and show that it outperforms or matches CLIP oracle performance by using just a single GPU desktop to actively query the Internet for 30–40 hours.}
}

@inproceedings{li2022internetcuriosity,
    abbr={ECCV SSLWIN},
    bibtex_show={true},
    title={Internet Curiosity: Directed Unsupervised Learning on Uncurated Internet Data},
    author={Li*, Alexander C. and Brown*, Ellis and Efros, Alexei A., and Pathak, Deepak},
    year={2022},
    booktitle = {European Conference on Computer Vision Workshop on ``Self Supervised Learning: What is Next?''},
    address = {Tel Aviv, Israel},
    abstract={We show that a curiosity-driven computer vision algorithm can learn to efficiently query Internet text-to-image search engines for images that improve the model's performance on a specified dataset. In contrast to typical self-supervised computer vision algorithms, which learn from static datasets, our model actively expands its training set with the most relevant images. First, we calculate the image-level curiosity reward as the negative distance of an image's representation to its nearest neighbor in the targeted dataset. This reward is easily estimated using only unlabeled data from the targeted dataset, and can be aggregated into a query-level reward that effectively identifies useful queries. Second, we use text embedding similarity scores to propagate observed curiosity rewards to untried text queries. This efficiently identifies relevant semantic clusters without any need for class labels or label names from the targeted dataset. Our method significantly outperforms models that require 1-2 orders of magnitude more compute and data.},
    poster={presentations/2022_ECCV_InternetCuriosity_Poster.pdf},
    preview={2022_eccv_sslwin.png},
    pdf={https://link.springer.com/chapter/10.1007/978-3-031-25069-9_7}
}

@article{brown2018stts,
    abbr={Adv. CogSys},
    bibtex_show={true},
    title={An Architecture for Spatiotemporal Template-Based Search},
    author={Brown, Ellis and Park, Soobeen, and Wardord, Noel and Seiffert, Adriane and Kawamura, Kazuhiko and Lappin, Joseph and Kunda, Maithilee},
    year={2018},
    journal = {Advances in Cognitive Systems},
    volume = {6},
    pages = {101--118},
    selected={true},
    abstract={Visual search for a spatiotemporal target occurs frequently in human experience---from military or aviation staff monitoring complex displays of multiple moving objects to daycare teachers monitoring a group of children on a playground for risky behaviors. In spatiotemporal search, unlike more traditional visual search tasks, the target cannot be identified from a single frame of visual experience; as the target is a spatiotemporal pattern that unfolds over time, detection of the target must also integrate information over time. We propose a new computational cognitive architecture used to model and understand human visual attention in the specific context of visual search for a spatiotemporal target. Results from a previous human participant study found that humans show interesting attentional capacity limitations in this type of search task.  Our architecture, called the SpatioTemporal Template-based Search (STTS) architecture, solves the same search task from the study using a wide variety of parameterized models that each represent a different cognitive theory of visual attention from the psychological literature.  We present results from initial computational experiments using STTS as a first step towards understanding the computational nature of attentional bottlenecks in this type of search task, and we discuss how continued STTS experiments will help determine which theoretical models best explain the capacity limitations shown by humans.  We expect that results from this research will help refine the design of visual information displays to help human operators perform difficult, real-world monitoring tasks.},
    pdf={http://cogsys.org/journal/volume6/article-6-8.pdf},
    preview={2018_stts.gif},
}

@inproceedings{brown2018spatiotemporal,
    abbr={ACS-18},
    author = {Brown, Ellis and Park, Soobeen and Warford, Noel and Seiffert, Adriane and Kawamura, Kazuhiko and Lappin, Joe and Kunda, Maithilee},
    title = {SpatioTemporal Template-based Search: An Architecture for Spatiotemporal Template-Based Search},
    booktitle = {Proceedings of the 6th Conference on Advances in Cognitive Systems},
    year = {2018},
    address = {Stanford, CA},
    month = {August},
    pdf = {https://my.vanderbilt.edu/aivaslab/files/2022/03/Brown-et-al-2018-SpatioTemporal-Template-based-Search-conference-paper.pdf},
    slides={https://docs.google.com/presentation/d/1YmzagDd5uLdEq42bkXCAX-vy0kdYvxeQp9jIYhFo-z8/present},
}
