---
---

@string{aps = {American Physical Society,}}

@article{li2023internet,
    abbr={preprint},
    title={Internet Explorer: Targeted Representation Learning on the Open Web},
    author={Li*, Alexander and Brown*, Ellis and Efros, Alexei A., and Pathak, Deepak},
    year={2023},
    journal={arXiv:cs.LG},
    selected={true},
    arxiv={2302.14051},
    website={https://internet-explorer-ssl.github.io/},
    pdf={https://internet-explorer-ssl.github.io/static/pdf/Internet_Explorer.pdf},
    preview={internet_explorer.gif},
    abstract={Vision models heavily rely on fine-tuning general-purpose models pre-trained on large, static datasets. These general-purpose models only understand knowledge within their pre-training datasets, which are tiny, out-of-date snapshots of the Internet—where billions of images are uploaded each day. We suggest an alternate approach: rather than hoping our static datasets transfer to our desired tasks after large-scale pre-training, we propose dynamically utilizing the Internet to quickly train a small-scale model that does extremely well on the task at hand. Our approach, called Internet Explorer, explores the web in a self-supervised manner to progressively find relevant examples that improve performance on a desired target dataset. It cycles between searching for images on the Internet with text queries, self-supervised training on downloaded images, determining which images were useful, and prioritizing what to search for next. We evaluate Internet Explorer across several datasets and show that it outperforms or matches CLIP oracle performance by using just a single GPU desktop to actively query the Internet for 30–40 hours.}
}

@inproceedings{li2022internetcuriosity,
    abbr={ECCV SSLWIN},
    bibtex_show={true},
    title={Internet Curiosity: Directed Unsupervised Learning on Uncurated Internet Data},
    author={Li*, Alexander and Brown*, Ellis and Efros, Alexei A., and Pathak, Deepak},
    year={2022},
    booktitle = {European Conference on Computer Vision Workshop on ``Self Supervised Learning: What is Next?''},
    address = {Tel Aviv, Israel},
    abstract={We show that a curiosity-driven computer vision algorithm can learn to efficiently query Internet text-to-image search engines for images that improve the model's performance on a specified dataset. In contrast to typical self-supervised computer vision algorithms, which learn from static datasets, our model actively expands its training set with the most relevant images. First, we calculate the image-level curiosity reward as the negative distance of an image's representation to its nearest neighbor in the targeted dataset. This reward is easily estimated using only unlabeled data from the targeted dataset, and can be aggregated into a query-level reward that effectively identifies useful queries. Second, we use text embedding similarity scores to propagate observed curiosity rewards to untried text queries. This efficiently identifies relevant semantic clusters without any need for class labels or label names from the targeted dataset. Our method significantly outperforms models that require 1-2 orders of magnitude more compute and data.},
    poster={presentations/2022_ECCV_InternetCuriosity_Poster.pdf},
    preview={2022_eccv_sslwin.png},
    pdf={https://link.springer.com/chapter/10.1007/978-3-031-25069-9_7}
}

@article{brown2018stts,
    abbr={Adv. CogSys},
    bibtex_show={true},
    title={An Architecture for Spatiotemporal Template-Based Search},
    author={Brown, Ellis and Park, Soobeen, and Wardord, Noel and Seiffert, Adriane and Kawamura, Kazuhiko and Lappin, Joseph and Kunda, Maithilee},
    year={2018},
    journal = {Advances in Cognitive Systems},
    volume = {6},
    pages = {101--118},
    selected={true},
    abstract={Visual search for a spatiotemporal target occurs frequently in human experience---from military or aviation staff monitoring complex displays of multiple moving objects to daycare teachers monitoring a group of children on a playground for risky behaviors. In spatiotemporal search, unlike more traditional visual search tasks, the target cannot be identified from a single frame of visual experience; as the target is a spatiotemporal pattern that unfolds over time, detection of the target must also integrate information over time. We propose a new computational cognitive architecture used to model and understand human visual attention in the specific context of visual search for a spatiotemporal target. Results from a previous human participant study found that humans show interesting attentional capacity limitations in this type of search task.  Our architecture, called the SpatioTemporal Template-based Search (STTS) architecture, solves the same search task from the study using a wide variety of parameterized models that each represent a different cognitive theory of visual attention from the psychological literature.  We present results from initial computational experiments using STTS as a first step towards understanding the computational nature of attentional bottlenecks in this type of search task, and we discuss how continued STTS experiments will help determine which theoretical models best explain the capacity limitations shown by humans.  We expect that results from this research will help refine the design of visual information displays to help human operators perform difficult, real-world monitoring tasks.},
    pdf={http://cogsys.org/journal/volume6/article-6-8.pdf},
    preview={2018_stts.gif},
}

@inproceedings{brown2018spatiotemporal,
    abbr={ACS-18},
    author = {Brown, Ellis and Park, Soobeen and Warford, Noel and Seiffert, Adriane and Kawamura, Kazuhiko and Lappin, Joe and Kunda, Maithilee},
    title = {SpatioTemporal Template-based Search: An Architecture for Spatiotemporal Template-Based Search},
    booktitle = {Proceedings of the 6th Conference on Advances in Cognitive Systems},
    year = {2018},
    address = {Stanford, CA},
    month = {August},
    pdf = {https://my.vanderbilt.edu/aivaslab/files/2022/03/Brown-et-al-2018-SpatioTemporal-Template-based-Search-conference-paper.pdf},
    slides={https://docs.google.com/presentation/d/1YmzagDd5uLdEq42bkXCAX-vy0kdYvxeQp9jIYhFo-z8/present},
}
