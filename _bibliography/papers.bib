---
---

@article{yang2025cambrian-s,
    author = {Yang*, Shusheng and Yang*, Jihan and Huang†, Pinzhi and Brown†, Ellis and Yang, Zihao and Yu, Yue and Tong, Shengbang and Zheng, Zihan and Xu, Yifan and Wang, Muhan and Lu, Danhao and Fergus, Rob and LeCun, Yann and Fei-Fei, Li and Xie, Saining},
    title = {{Cambrian-S: Towards Spatial Supersensing in Video}},
    journal = {arXiv preprint arXiv:2511.04670},
    year = {2025},
    arxiv={2511.04670},
    pdf={https://arxiv.org/pdf/2511.04670},
    abbr={arXiv},
    website={https://cambrian-mllm.github.io/cambrian-s/},
    include={true},
    bibtex_show={true},
    selected={true},
    preview={cambrian-s.png},
    annotation={† Core team},
}

@article{brown2025shortcuts,
    author = {Brown, Ellis and Yang, Jihan and Yang, Shusheng and Fergus, Rob and Xie, Saining},
    title = {Benchmark Designers Should ``Train on the Test Set'' to Expose Exploitable Non-Visual Shortcuts},
    journal = {arXiv preprint arXiv:2511.04655},
    year = {2025},
    arxiv={2511.04655},
    pdf={https://arxiv.org/pdf/2511.04655},
    abbr={arXiv},
    website={https://vision-x-nyu.github.io/test-set-training},
    include={true},
    bibtex_show={true},
    selected={true},
    preview={TsT.png},
}


@article{brown2025simsv,
    title   =  {{SIMS-V}: Simulated Instruction-Tuning for Spatial Video Understanding},
    author  =  {Brown, Ellis and Ray, Arijit and Krishna, Ranjay and Girshick, Ross and Fergus, Rob and Xie, Saining},
    journal =  {arXiv preprint arXiv:2511.04668},
    year    =  {2025},
    arxiv={2511.04668},
    pdf={https://arxiv.org/pdf/2511.04668},
    abbr={arXiv},
    website={./sims-v/},
    include={true},
    bibtex_show={true},
    selected={true},
    preview={sims-v.png},
}


@InProceedings{ray2025satdynamicspatialaptitude,
    title={SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models}, 
    author={Arijit Ray and Jiafei Duan† and Ellis Brown† and Reuben Tan and Dina Bashkirova and Rose Hendrix and Kiana Ehsani and Aniruddha Kembhavi and Bryan A. Plummer and Ranjay Krishna and Kuo-Hao Zeng and Kate Saenko},
    year={2025},
    arxiv={2412.07755},
    booktitle={COLM},
    include={true},
    abbr={COLM},
    website={https://arijitray.com/SAT/},
    pdf={https://arxiv.org/pdf/2412.07755},
    abstract={Reasoning about motion and space is a fundamental cognitive capability that is required by multiple real-world applications. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only focus on static spatial relationships, and not dynamic awareness of motion and space, i.e., reasoning about the effect of egocentric and object motions on spatial relationships. Manually annotating such object and camera movements is expensive. Hence, we introduce SAT, a simulated spatial aptitude training dataset comprising both static and dynamic spatial reasoning across 175K question-answer (QA) pairs and 20K scenes. Complementing this, we also construct a small (150 image-QAs) yet challenging dynamic spatial test set using real-world images. Leveraging our SAT datasets and 6 existing static spatial benchmarks, we systematically investigate what improves both static and dynamic spatial awareness. Our results reveal that simulations are surprisingly effective at imparting spatial aptitude to MLMs that translate to real images. We show that perfect annotations in simulation are more effective than existing approaches of pseudo-annotating real images. For instance, SAT training improves a LLaVA-13B model by an average 11% and a LLaVA-Video-7B model by an average 8% on multiple spatial benchmarks, including our real-image dynamic test set and spatial reasoning on long videos -- even outperforming some large proprietary models. While reasoning over static relationships improves with synthetic training data, there is still considerable room for improvement for dynamic reasoning questions.},
    preview={SAT.jpg},
    bibtex_show={true},
    annotation={† Joint second author},
}


@InProceedings{tong2024cambrian,
    abbr={NeurIPS},
    title={{Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs}},
    author={Tong*, Shengbang and Brown*, Ellis and Wu*, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng, and Iyer, Adithya and Pan, Xichen and Wang, Ziteng and Fergus, Rob and LeCun, Yann and Xie, Saining},
    year={2024},
    arxiv={2406.16860},
    website={https://cambrian-mllm.github.io/},
    pdf={https://arxiv.org/pdf/2406.16860},
    code={https://github.com/cambrian-mllm/cambrian},
    abstract={We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures---self-supervised, strongly supervised, or combinations thereof---based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, addressing the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.},
    bibtex_show={true},
    preview={cambrian.png},
    selected={true},
    include={true},
    booktitle={NeurIPS},
    annotation={* Equal contribution},
    award_name={Oral <strong>(1.8%)</strong>},
    award={Selected for Oral Presentation <strong>(1.8%)</strong> at NeurIPS 2024},
}
% google_scholar_id={LkGwnXOMwfcC},

@InProceedings{yang2024virl,
    author={Yang, Jihan and Ding, Runyu and Brown, Ellis and Qi, Xiaojuan and Xie, Saining},
    title={V-IRL: Grounding Virtual Intelligence in Real Life},
    arxiv={2402.03310},
    year={2024},
    website={https://virl-platform.github.io/},
    pdf={https://virl-platform.github.io/static/V-IRL.pdf},
    code={https://github.com/VIRL-Platform/VIRL},
    abstract={There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as ﬂexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.},
    bibtex_show={true},
    include={true},
    abbr={ECCV},
    preview={virl_2.png},
    selected={false},
    booktitle={ECCV},
}


@mastersthesis{brown2023online,
    author       = {Ellis Brown},
    title        = {Online Representation Learning on the Open Web},
    school       = {Carnegie Mellon University},
    note         = {Master's Thesis. Committee: Deepak Pathak, Alexei Efros, and Deva Ramanan},
    year         = {2023},
    abbr         = {Thesis},
    pdf          = {https://ellisbrown.github.io/assets/pdf/papers/2023_CMU_Masters_Thesis.pdf},
    slides       = {https://ellisbrown.github.io/assets/pdf/presentations/2023_CMU_Masters_Defense.pdf},
    video        = {https://youtu.be/haT48f2TPxs},
    code         = {https://github.com/ellisbrown/cmu-masters-thesis},
    bibtex_show  = {true},
}


@InProceedings{li2023diffusion,
    abbr={ICCV},
    bibtex_show={true},
    title={Your Diffusion Model is Secretly a Zero-Shot Classifier}, 
    author={Alexander C. Li and Mihir Prabhudesai and Shivam Duggal and Ellis Brown and Deepak Pathak},
    year={2023},
    include={true},
    booktitle = {ICCV},
    year      = {2023},
    pages     = {2206-2217},
    arxiv     = {2303.16203},
    website={https://diffusion-classifier.github.io/},
    pdf={https://diffusion-classifier.github.io/static/docs/DiffusionClassifier.pdf},
    preview={diffusion_classifier.jpg},
    abstract={The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation.
    <br><br>
    In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification, which we call Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, our diffusion-based approach has significantly stronger multimodal compositional reasoning ability than competing discriminative approaches.
    <br><br>
    Finally, we use Diffusion Classifier to extract standard classifiers from class-conditional diffusion models trained on ImageNet. Our models achieve strong classification performance using only weak augmentations and exhibit qualitatively better "effective robustness" to distribution shift. Overall, our results are a step toward using generative over discriminative models for downstream tasks.},
    selected={false},
}

@inproceedings{li2023internet,
    abbr={ICML},
    bibtex_show={true},
    title={Internet Explorer: Targeted Representation Learning on the Open Web},
    author={Li*, Alexander C. and Brown*, Ellis and Efros, Alexei A., and Pathak, Deepak},
    year={2023},
    booktitle={ICML},
    selected={true},
    include={true},
    arxiv={2302.14051},
    website={https://internet-explorer-ssl.github.io/},
    pdf={https://internet-explorer-ssl.github.io/static/docs/InternetExplorer.pdf},
    preview={internet_explorer.gif},
    abstract={Vision models heavily rely on fine-tuning general-purpose models pre-trained on large, static datasets. These general-purpose models only understand knowledge within their pre-training datasets, which are tiny, out-of-date snapshots of the Internet—where billions of images are uploaded each day. We suggest an alternate approach: rather than hoping our static datasets transfer to our desired tasks after large-scale pre-training, we propose dynamically utilizing the Internet to quickly train a small-scale model that does extremely well on the task at hand. Our approach, called Internet Explorer, explores the web in a self-supervised manner to progressively find relevant examples that improve performance on a desired target dataset. It cycles between searching for images on the Internet with text queries, self-supervised training on downloaded images, determining which images were useful, and prioritizing what to search for next. We evaluate Internet Explorer across several datasets and show that it outperforms or matches CLIP oracle performance by using just a single GPU desktop to actively query the Internet for 30–40 hours.}
}

@inproceedings{li2022internetcuriosity,
    abbr={ECCV},
    bibtex_show={true},
    title={Internet Curiosity: Directed Unsupervised Learning on Uncurated Internet Data},
    author={Li*, Alexander C. and Brown*, Ellis and Efros, Alexei A., and Pathak, Deepak},
    year={2022},
    booktitle = {ECCV Workshop on ``Self Supervised Learning: What is Next?''},
    address = {Tel Aviv, Israel},
    abstract={We show that a curiosity-driven computer vision algorithm can learn to efficiently query Internet text-to-image search engines for images that improve the model's performance on a specified dataset. In contrast to typical self-supervised computer vision algorithms, which learn from static datasets, our model actively expands its training set with the most relevant images. First, we calculate the image-level curiosity reward as the negative distance of an image's representation to its nearest neighbor in the targeted dataset. This reward is easily estimated using only unlabeled data from the targeted dataset, and can be aggregated into a query-level reward that effectively identifies useful queries. Second, we use text embedding similarity scores to propagate observed curiosity rewards to untried text queries. This efficiently identifies relevant semantic clusters without any need for class labels or label names from the targeted dataset. Our method significantly outperforms models that require 1-2 orders of magnitude more compute and data.},
    poster={presentations/2022_ECCV_InternetCuriosity_Poster.pdf},
    preview={2022_eccv_sslwin.png},
    pdf={https://link.springer.com/chapter/10.1007/978-3-031-25069-9_7}
}

@article{brown2018stts,
    abbr={CogSys},
    bibtex_show={true},
    title={An Architecture for Spatiotemporal Template-Based Search},
    author={Brown, Ellis and Park, Soobeen, and Wardord, Noel and Seiffert, Adriane and Kawamura, Kazuhiko and Lappin, Joseph and Kunda, Maithilee},
    year={2018},
    journal = {Advances in Cognitive Systems},
    volume = {6},
    pages = {101--118},
    selected={false},
    abstract={Visual search for a spatiotemporal target occurs frequently in human experience---from military or aviation staff monitoring complex displays of multiple moving objects to daycare teachers monitoring a group of children on a playground for risky behaviors. In spatiotemporal search, unlike more traditional visual search tasks, the target cannot be identified from a single frame of visual experience; as the target is a spatiotemporal pattern that unfolds over time, detection of the target must also integrate information over time. We propose a new computational cognitive architecture used to model and understand human visual attention in the specific context of visual search for a spatiotemporal target. Results from a previous human participant study found that humans show interesting attentional capacity limitations in this type of search task.  Our architecture, called the SpatioTemporal Template-based Search (STTS) architecture, solves the same search task from the study using a wide variety of parameterized mod els that each represent a different cognitive theory of visual attention from the psychological literature.  We present results from initial computational experiments using STTS as a first step towards understanding the computational nature of attentional bottlenecks in this type of search task, and we discuss how continued STTS experiments will help determine which theoretical models best explain the capacity limitations shown by humans.  We expect that results from this research will help refine the design of visual information displays to help human operators perform difficult, real-world monitoring tasks.},
    pdf={http://cogsys.org/journal/volume6/article-6-8.pdf},
    preview={2018_stts.gif},
}

@inproceedings{brown2018spatiotemporal,
    abbr={CogSys},
    author = {Brown, Ellis and Park, Soobeen and Warford, Noel and Seiffert, Adriane and Kawamura, Kazuhiko and Lappin, Joe and Kunda, Maithilee},
    title = {SpatioTemporal Template-based Search: An Architecture for Spatiotemporal Template-Based Search},
    booktitle = {Advances in Cognitive Systems},
    year = {2018},
    address = {Stanford, CA},
    month = {August},
    pdf = {https://my.vanderbilt.edu/aivaslab/files/2022/03/Brown-et-al-2018-SpatioTemporal-Template-based-Search-conference-paper.pdf},
    slides={https://docs.google.com/presentation/d/1YmzagDd5uLdEq42bkXCAX-vy0kdYvxeQp9jIYhFo-z8/present},
}
