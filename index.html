<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Ellis Brown </title> <meta name="author" content="Ellis L. Brown II"> <meta name="description" content=""> <meta name="keywords" content="self-supervised learning, representation learning, multimodal, vision-language, generalization, curiosity, deep learning, computer vision, machine learning, artificial intelligence"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%B4&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ellisbrown.github.io//"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">research </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <article> <div class="profile center justify-content-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/elb-800.webp 800w,/assets/img/elb-1400.webp 1400w," type="image/webp" sizes="95vw"> <img src="/assets/img/elb.jpg" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="elb.jpg" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="header" style="margin-bottom: 1rem;"> <div class="name"> Ellis Brown </div> <div class="title"> PhD Student @ NYU Courant </div> <div class="address" style="font-size: large;"> <p>ellis.brown at nyu dot edu</p> </div> <div class="contact-icons"> <a href="https://github.com/ellisbrown" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/ellislbrownii" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://scholar.google.com/citations?user=Hp5uEnUAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/_ellisbrown" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note"> </div> </div> </div> </article> </div> <div class="clearfix"> <p>I am a CS PhD Student at <a href="https://cs.nyu.edu/" rel="external nofollow noopener" target="_blank">NYU Courant</a> advised by Profs. <a href="https://www.sainingxie.com/" rel="external nofollow noopener" target="_blank">Saining Xie</a> and <a href="https://cs.nyu.edu/~fergus" rel="external nofollow noopener" target="_blank">Rob Fergus</a>. My research is supported by the <a href="https://www.ndseg.org/" rel="external nofollow noopener" target="_blank">NDSEG Fellowship</a>. I have interned at <a href="https://ai.meta.com/research/" rel="external nofollow noopener" target="_blank">Meta FAIR</a> (w/ <a href="https://jasonqsy.github.io" rel="external nofollow noopener" target="_blank">Shengyi Qian</a>) and at <a href="https://prior.allenai.org/" rel="external nofollow noopener" target="_blank">Ai2 PRIOR</a> (w/ <a href="https://www.rossgirshick.info/" rel="external nofollow noopener" target="_blank">Ross Girshick</a>). </p> <p>Before NYU, I graduated from a Master’s at <a href="https://www.cmu.edu/" rel="external nofollow noopener" target="_blank">Carnegie Mellon</a> where I was advised by Profs. <a href="http://www.cs.cmu.edu/~dpathak/" rel="external nofollow noopener" target="_blank">Deepak Pathak</a> and <a href="http://www.cs.berkeley.edu/~efros/" rel="external nofollow noopener" target="_blank">Alyosha Efros</a>. Before that, I was a founding research engineer at <a href="http://www.blackrock.com/ai" rel="external nofollow noopener" target="_blank">BlackRock AI Labs</a>, working with Profs. <a href="http://mykel.kochenderfer.com/" rel="external nofollow noopener" target="_blank">Mykel Kochenderfer</a>, <a href="http://web.stanford.edu/~boyd/" rel="external nofollow noopener" target="_blank">Stephen Boyd</a>, and <a href="https://hastie.su.domains/" rel="external nofollow noopener" target="_blank">Trevor Hastie</a> on applied research &amp; finance and a non-degree grad student at <a href="https://www.cs.stanford.edu/" rel="external nofollow noopener" target="_blank">Stanford</a> and <a href="https://www.cs.columbia.edu/" rel="external nofollow noopener" target="_blank">Columbia</a>. I did my undergrad at <a href="http://www.vanderbilt.edu" rel="external nofollow noopener" target="_blank">Vanderbilt</a> where I majored in CS &amp; Math and did research in CogSci &amp; Vision with Prof. <a href="http://my.vanderbilt.edu/mkunda/" rel="external nofollow noopener" target="_blank">Maithilee Kunda</a>. I’m originally from St. Louis, MO and am a proud member of the <a href="http://www.osagenation-nsn.gov/" rel="external nofollow noopener" target="_blank">Osage Nation</a>.</p> <p>→ If you haven’t made time for a regular checkin with a doctor recently, <em><a href="/blog/2020/make-time-for-the-doctor/">please do!</a></em>   Even if you feel perfectly healthy.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 10%">May., 2025</th> <td> Honored to be recognized as a <a href="https://cvpr.thecvf.com/Conferences/2025/ProgramCommittee#all-outstanding-reviewer" rel="external nofollow noopener" target="_blank">CVPR 2025 Outstanding Reviewer</a>! </td> </tr> <tr> <th scope="row" style="width: 10%">Sep., 2024</th> <td> <a href="https://cambrian-mllm.github.io/" rel="external nofollow noopener" target="_blank">Cambrian</a> was accepted to <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS 2024</a> as an oral presentation 🪼🎉 </td> </tr> <tr> <th scope="row" style="width: 10%">Mar., 2024</th> <td> Thrilled to have been awarded the <a href="https://ndseg.org/" rel="external nofollow noopener" target="_blank">NDSEG Fellowship</a> to support my PhD research at NYU! </td> </tr> <tr> <th scope="row" style="width: 10%">Feb., 2024</th> <td> I will be joining <a href="https://allenai.org/" rel="external nofollow noopener" target="_blank">AllenAI (AI2)</a> as a Resesarch Intern this summer in Seattle, working with <a href="https://www.rossgirshick.info/" rel="external nofollow noopener" target="_blank">Ross Girshick</a>! </td> </tr> <tr> <th scope="row" style="width: 10%">Aug., 2023</th> <td> Excited to be starting my PhD at <a href="https://wp.nyu.edu/cilvr/" rel="external nofollow noopener" target="_blank">NYU</a> advised by Profs. <a href="https://www.sainingxie.com/" rel="external nofollow noopener" target="_blank">Saining Xie</a> and <a href="https://cs.nyu.edu/~fergus/" rel="external nofollow noopener" target="_blank">Rob Fergus</a> 🎉🗽 </td> </tr> </table> </div> </div> <div class="publications"> <h2>research <a href="/research">(all)</a> </h2> <p></p> <p>My research interests lie at the intersection of deep learning, computer vision, and robotics—particularly in the areas of (multimodal) representation learning, self-supervised learning, open-endedness, and agents.</p> <h4>publications</h4> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row "> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">COLM</abbr> <figure> <picture> <img src="/assets/img/publication_preview/SAT.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="SAT.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ray2025satdynamicspatialaptitude" class="col-sm-8"> <div class="title">SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models</div> <div class="author"> <a href="https://arijitray1993.github.io" rel="external nofollow noopener" target="_blank">Arijit Ray</a>, Jiafei Duan<sup>†</sup>, <em>Ellis Brown<sup>†</sup></em>, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, and Kate Saenko <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† Joint second author"> </i> </div> <div class="periodical"> <em>In COLM</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2412.07755" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2412.07755" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://arijitray.com/SAT/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Reasoning about motion and space is a fundamental cognitive capability that is required by multiple real-world applications. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only focus on static spatial relationships, and not dynamic awareness of motion and space, i.e., reasoning about the effect of egocentric and object motions on spatial relationships. Manually annotating such object and camera movements is expensive. Hence, we introduce SAT, a simulated spatial aptitude training dataset comprising both static and dynamic spatial reasoning across 175K question-answer (QA) pairs and 20K scenes. Complementing this, we also construct a small (150 image-QAs) yet challenging dynamic spatial test set using real-world images. Leveraging our SAT datasets and 6 existing static spatial benchmarks, we systematically investigate what improves both static and dynamic spatial awareness. Our results reveal that simulations are surprisingly effective at imparting spatial aptitude to MLMs that translate to real images. We show that perfect annotations in simulation are more effective than existing approaches of pseudo-annotating real images. For instance, SAT training improves a LLaVA-13B model by an average 11% and a LLaVA-Video-7B model by an average 8% on multiple spatial benchmarks, including our real-image dynamic test set and spatial reasoning on long videos – even outperforming some large proprietary models. While reasoning over static relationships improves with synthetic training data, there is still considerable room for improvement for dynamic reasoning questions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ray2025satdynamicspatialaptitude</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ray, Arijit and Duan, Jiafei and Brown, Ellis and Tan, Reuben and Bashkirova, Dina and Hendrix, Rose and Ehsani, Kiana and Kembhavi, Aniruddha and Plummer, Bryan A. and Krishna, Ranjay and Zeng, Kuo-Hao and Saenko, Kate}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">include</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{COLM}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row selected-publication"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#8c47d1"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/cambrian.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cambrian.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tong2024cambrian" class="col-sm-8"> <div class="title">Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</div> <div class="author"> <a href="https://tsb0601.github.io/petertongsb/" rel="external nofollow noopener" target="_blank">Shengbang Tong<sup>*</sup></a>, <em>Ellis Brown<sup>*</sup></em>, Penghao Wu<sup>*</sup>, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, <a href="https://jihanyang.github.io/" rel="external nofollow noopener" target="_blank">Jihan Yang</a> , <a href="https://github.com/vealocia" rel="external nofollow noopener" target="_blank">Shusheng Yang</a>, Adithya Iyer, Xichen Pan, Ziteng Wang, <a href="https://cs.nyu.edu/~fergus/" rel="external nofollow noopener" target="_blank">Rob Fergus</a>, <a href="http://yann.lecun.com/" rel="external nofollow noopener" target="_blank">Yann LeCun</a>, and <a href="https://www.sainingxie.com/" rel="external nofollow noopener" target="_blank">Saining Xie</a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* Equal contribution"> </i> </div> <div class="periodical"> <em>In NeurIPS</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button"> 🎖️  Oral <strong>(1.8%)</strong></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2406.16860" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2406.16860" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/cambrian-mllm/cambrian" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://cambrian-mllm.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Selected for Oral Presentation <strong>(1.8%)</strong> at NeurIPS 2024</p> </div> <div class="abstract hidden"> <p>We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures—self-supervised, strongly supervised, or combinations thereof—based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, addressing the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tong2024cambrian</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and Wang, Ziteng and Fergus, Rob and LeCun, Yann and Xie, Saining}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">include</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NeurIPS}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row "> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#5887b8"> <a href="https://eccv.ecva.net/" rel="external nofollow noopener" target="_blank">ECCV</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/virl_2.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="virl_2.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yang2024virl" class="col-sm-8"> <div class="title">V-IRL: Grounding Virtual Intelligence in Real Life</div> <div class="author"> <a href="https://jihanyang.github.io/" rel="external nofollow noopener" target="_blank">Jihan Yang</a>, Runyu Ding, <em>Ellis Brown</em>, Xiaojuan Qi, and <a href="https://www.sainingxie.com/" rel="external nofollow noopener" target="_blank">Saining Xie</a> </div> <div class="periodical"> <em>In ECCV</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.03310" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://virl-platform.github.io/static/V-IRL.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/VIRL-Platform/VIRL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://virl-platform.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as ﬂexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yang2024virl</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Jihan and Ding, Runyu and Brown, Ellis and Qi, Xiaojuan and Xie, Saining}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{V-IRL: Grounding Virtual Intelligence in Real Life}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">include</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row "> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#5887b8"> <a href="https://iccv.thecvf.com/" rel="external nofollow noopener" target="_blank">ICCV</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/diffusion_classifier.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="diffusion_classifier.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2023diffusion" class="col-sm-8"> <div class="title">Your Diffusion Model is Secretly a Zero-Shot Classifier</div> <div class="author"> <a href="https://alexanderli.com" rel="external nofollow noopener" target="_blank">Alexander C. Li</a>, <a href="https://mihirp1998.github.io/" rel="external nofollow noopener" target="_blank">Mihir Prabhudesai</a>, <a href="https://shivamduggal4.github.io/" rel="external nofollow noopener" target="_blank">Shivam Duggal</a>, <em>Ellis Brown</em>, and <a href="https://www.cs.cmu.edu/~dpathak" rel="external nofollow noopener" target="_blank">Deepak Pathak</a> </div> <div class="periodical"> <em>In ICCV</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2303.16203" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://diffusion-classifier.github.io/static/docs/DiffusionClassifier.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://diffusion-classifier.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. <br><br> In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification, which we call Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, our diffusion-based approach has significantly stronger multimodal compositional reasoning ability than competing discriminative approaches. <br><br> Finally, we use Diffusion Classifier to extract standard classifiers from class-conditional diffusion models trained on ImageNet. Our models achieve strong classification performance using only weak augmentations and exhibit qualitatively better "effective robustness" to distribution shift. Overall, our results are a step toward using generative over discriminative models for downstream tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2023diffusion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Your Diffusion Model is Secretly a Zero-Shot Classifier}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Alexander C. and Prabhudesai, Mihir and Duggal, Shivam and Brown, Ellis and Pathak, Deepak}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">include</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICCV}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2206-2217}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row selected-publication"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c30916"> <a href="https://icml.cc" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/internet_explorer.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="internet_explorer.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2023internet" class="col-sm-8"> <div class="title">Internet Explorer: Targeted Representation Learning on the Open Web</div> <div class="author"> <a href="https://alexanderli.com" rel="external nofollow noopener" target="_blank">Alexander C. Li<sup>*</sup></a>, <em>Ellis Brown<sup>*</sup></em>, <a href="https://www.cs.berkeley.edu/~efros/" rel="external nofollow noopener" target="_blank">Alexei A. Efros</a>, and <a href="https://www.cs.cmu.edu/~dpathak" rel="external nofollow noopener" target="_blank">Deepak Pathak</a> </div> <div class="periodical"> <em>In ICML</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2302.14051" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://internet-explorer-ssl.github.io/static/docs/InternetExplorer.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://internet-explorer-ssl.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Vision models heavily rely on fine-tuning general-purpose models pre-trained on large, static datasets. These general-purpose models only understand knowledge within their pre-training datasets, which are tiny, out-of-date snapshots of the Internet—where billions of images are uploaded each day. We suggest an alternate approach: rather than hoping our static datasets transfer to our desired tasks after large-scale pre-training, we propose dynamically utilizing the Internet to quickly train a small-scale model that does extremely well on the task at hand. Our approach, called Internet Explorer, explores the web in a self-supervised manner to progressively find relevant examples that improve performance on a desired target dataset. It cycles between searching for images on the Internet with text queries, self-supervised training on downloaded images, determining which images were useful, and prioritizing what to search for next. We evaluate Internet Explorer across several datasets and show that it outperforms or matches CLIP oracle performance by using just a single GPU desktop to actively query the Internet for 30–40 hours.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2023internet</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Internet Explorer: Targeted Representation Learning on the Open Web}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Alexander C. and Brown, Ellis and Efros, Alexei A. and Pathak, Deepak}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICML}</span><span class="p">,</span>
  <span class="na">include</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <br><br><br> <div class="social"> <div class="contact-icons"> <a href="https://github.com/ellisbrown" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/ellislbrownii" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://scholar.google.com/citations?user=Hp5uEnUAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/_ellisbrown" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note"></div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Ellis L. Brown II. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: October 30, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script src="/assets/js/tooltips-setup.js?53023e960fbc64cccb90d32e9363de2b"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-00N9EWF2WT"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-00N9EWF2WT');
  </script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>